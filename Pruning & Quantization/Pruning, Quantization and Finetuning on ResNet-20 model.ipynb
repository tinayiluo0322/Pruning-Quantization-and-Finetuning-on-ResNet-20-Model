{"cells":[{"cell_type":"markdown","source":["# Pruning, Quantization and Finetuning on ResNet-20 model\n","\n","### Luopeiwen Yi"],"metadata":{"id":"MOrMvs7q0rEh"}},{"cell_type":"markdown","source":["### 1. Introduction\n","Deep neural networks (DNNs) often require significant computation and memory resources, making them challenging to deploy on edge or embedded systems. To address this, model compression techniques such as pruning and fixed-point quantization are widely used. In this experiment, we explore different pruning strategies and quantization methods, both individually and in combination, to evaluate their effectiveness on the ResNet-20 model trained on CIFAR-10.\n","\n","---\n","\n","### 2. Experiment Design\n","\n","#### 2.1. Baseline\n","We start with a pretrained floating-point ResNet-20 model and evaluate its baseline performance:\n","- **Test Accuracy:** 0.9151\n","- **Test Loss:** 0.3231\n","\n","#### 2.2. Pruning Methods\n","We evaluate three pruning strategies, each aiming for 80% sparsity:\n","- **One-Shot Layer-Wise Pruning:** Prune 80% of the weights once, then fine-tune.\n","- **Iterative Layer-Wise Pruning:** Gradually prune over the first 10 epochs, increasing sparsity by 8% each epoch, followed by 10 epochs of fine-tuning.\n","- **Global Iterative Pruning:** Use a global threshold across all layers for pruning (same percent overall, variable per-layer sparsity), repeated in an iterative fashion.\n","\n","#### 2.3. Quantization Methods\n","We apply fixed-point quantization to the residual blocks of the ResNet-20 model using:\n","- **Asymmetric Quantization**\n","- **Symmetric Quantization**\n","- **Both with and without fine-tuning**\n","\n","Bit-widths (Nbits) tested: 6, 5, 4, 3, 2.\n","\n","#### 2.4. Combined Pruning + Quantization\n","We examine the performance of applying fixed-point quantization (Nbits=5 to 2) on a model pruned to 80% sparsity using the best-performing pruning method, and evaluate both before and after finetuning.\n","\n","---\n","\n","### 3. Results and Observations\n","\n","#### 3.1. Pruning + Fine-Tuning Accuracy\n","| Method                           | Test Accuracy | Test Loss |\n","|----------------------------------|----------------|------------|\n","| Floating-point Baseline          | 0.9151         | 0.3231     |\n","| One-Shot Pruning + Fine-tuning   | 0.8794         | 0.3664     |\n","| Iterative Pruning + Fine-tuning  | 0.8769         | 0.3750     |\n","| Global Iterative Pruning + FT    | **0.8841**     | **0.3483** |\n","\n","**Observation:**\n","- All pruning methods incur accuracy drops from the FP baseline.\n","- Global iterative pruning performs best, balancing sparsity and accuracy.\n","- One-shot pruning is more aggressive and loses more performance.\n","\n","#### 3.2. Quantization without Finetuning\n","| Nbits | Asymmetric Acc | Symmetric Acc |\n","|-------|----------------|----------------|\n","| 6     | 0.9144         | 0.9134         |\n","| 5     | 0.9113         | 0.9071         |\n","| 4     | 0.8973         | 0.8532         |\n","| 3     | 0.7660         | 0.7151         |\n","| 2     | 0.0899         | 0.1000         |\n","\n","**Observation:**\n","- Both quantization types perform similarly at higher bit-widths.\n","- Symmetric quantization tends to degrade more quickly under low bit-width.\n","- Performance drops sharply at 3 and 2 bits, showing the need for finetuning.\n","\n","#### 3.3. Asymmetric Quantization with Finetuning\n","| Nbits | Accuracy After Finetune |\n","|-------|--------------------------|\n","| 5     | 0.9156                   |\n","| 4     | 0.9140                   |\n","| 3     | 0.9058                   |\n","| 2     | 0.8597                   |\n","\n","**Observation:**\n","- Finetuning significantly recovers performance, especially at lower precisions.\n","- At 5 and 4 bits, accuracy approaches or matches full-precision baseline.\n","\n","#### 3.4. Quantized Pruned Model\n","| Nbits | Acc Before FT | Acc After FT |\n","|-------|---------------|---------------|\n","| 5     | 0.8778        | 0.9032        |\n","| 4     | 0.8603        | 0.8994        |\n","| 3     | 0.7186        | 0.8715        |\n","| 2     | 0.1000        | 0.3348        |\n","\n","**Observation:**\n","- Finetuning is crucial when combining pruning and quantization.\n","- Accuracy recovers well up to 3-bit precision even after pruning.\n","- Performance degrades drastically at 2 bits.\n","\n","---\n","\n","### 4. Comparative Analysis\n","\n","| Feature                      | One-Shot Pruning | Iterative Pruning | Global Iterative | Quantization Only | Prune + Quantize |\n","|-----------------------------|------------------|-------------------|------------------|-------------------|------------------|\n","| Performance vs. FP Baseline | ↓ -3.9%        | ↓ -4.2%         | **↓ -3.4%**     | ↓ varies by Nbits | ↓ more at low bits |\n","| Sparsity Flexibility        | Low              | Medium            | High             | N/A               | High             |\n","| Bit-width Impact            | N/A              | N/A               | N/A              | High              | High             |\n","| Best Tradeoff               | No               | No                | **Yes**          | Yes (with FT)     | Yes (with FT)    |\n","\n","---\n","\n","### 5. Conclusion\n","This report demonstrates the effectiveness of pruning and quantization in compressing neural networks with minimal loss in performance. Key takeaways include:\n","- Global iterative pruning outperforms other sparsity strategies under high compression.\n","- Asymmetric quantization performs slightly better than symmetric at very low bit-widths.\n","- Finetuning is essential for recovering performance, especially after aggressive pruning or low-bit quantization.\n","- A combined pruning + quantization pipeline, with proper fine-tuning, can significantly reduce model size while maintaining acceptable accuracy.\n","\n","These findings support the feasibility of deploying compressed ResNet-20 models in resource-constrained environments without sacrificing much performance."],"metadata":{"id":"GTJ5TUTxSJnH"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"As7E8UUzxqth","executionInfo":{"status":"ok","timestamp":1743357752087,"user_tz":240,"elapsed":34764,"user":{"displayName":"Luopeiwen Yi","userId":"10397336716270013892"}},"outputId":"3213b17d-f9e0-49b0-a02b-dad069affaca"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import sys\n","import os"],"metadata":{"id":"gn67hCMAytUx","executionInfo":{"status":"ok","timestamp":1743358334739,"user_tz":240,"elapsed":42,"user":{"displayName":"Luopeiwen Yi","userId":"10397336716270013892"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Change this to the absolute path where dataset.py and utils.py are stored\n","CODE_PATH = \"/content/drive/MyDrive/ECE661 Assignment4\"\n","\n","# Add this path to sys.path so Python can find it\n","sys.path.append(CODE_PATH)\n","\n","# Check if Colab can see the files\n","print(\"Files in directory:\", os.listdir(CODE_PATH))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xKHtJzsqyvW9","executionInfo":{"status":"ok","timestamp":1743358337074,"user_tz":240,"elapsed":40,"user":{"displayName":"Luopeiwen Yi","userId":"10397336716270013892"}},"outputId":"559199cb-8073-4752-c179-c2198a6cc770"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Files in directory: ['train_util.py', 'resnet20.py', 'ECE661__Pruning_and_Fixed_Point_Quantization_HW4.pdf', 'pretrained_model.pt', '__pycache__', 'net_after_finetune.pt', 'net_after_iterative_prune.pt', 'net_after_global_iterative_prune.pt', 'FP_layers_template.py', 'quantized_net_after_finetune_Nbits_5.pt', 'quantized_net_after_finetune_Nbits_4.pt', 'quantized_net_after_finetune_Nbits_3.pt', 'quantized_net_after_finetune_Nbits_2.pt', 'pruned_quantized_net_Nbits_5.pt', 'pruned_quantized_net_Nbits_4.pt', 'pruned_quantized_net_Nbits_3.pt', 'pruned_quantized_net_Nbits_2.pt', 'FP_layers.py', 'FP_layers_asymmetric.py', 'Pruning, Fixed-point quantization and finetuning on ResNet-20 model.ipynb']\n"]}]},{"cell_type":"markdown","metadata":{"id":"ztcoi9s0EuUy"},"source":["### Model preperation"]},{"cell_type":"code","execution_count":4,"metadata":{"pycharm":{"is_executing":false},"id":"xsiyL6_uEuUy","executionInfo":{"status":"ok","timestamp":1743358377205,"user_tz":240,"elapsed":13660,"user":{"displayName":"Luopeiwen Yi","userId":"10397336716270013892"}}},"outputs":[],"source":["from resnet20 import ResNetCIFAR\n","from train_util import train, finetune, test\n","import torch\n","import numpy as np\n","import random\n","\n","import time\n","\n","import torchvision.transforms as transforms\n","import torchvision\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","from FP_layers import *\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"code","source":["seed = 42\n","random.seed(seed)  # Python's random module\n","np.random.seed(seed)  # NumPy's random module\n","torch.manual_seed(seed)  # PyTorch's random seed for CPU\n","torch.cuda.manual_seed(seed)  # PyTorch's random seed for the current GPU\n","torch.cuda.manual_seed_all(seed)  # PyTorch's random seed for all GPUs (if using multi-GPU)\n","\n","# Ensure deterministic behavior on GPU (optional, may slow down training)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","# Optional: Set environment variables for further reproducibility\n","os.environ['PYTHONHASHSEED'] = str(seed)"],"metadata":{"id":"cBQIjqCR9inm","executionInfo":{"status":"ok","timestamp":1743358799636,"user_tz":240,"elapsed":65,"user":{"displayName":"Luopeiwen Yi","userId":"10397336716270013892"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"pycharm":{"is_executing":false,"name":"#%%\n"},"id":"jQUz0zvZEuUz","executionInfo":{"status":"ok","timestamp":1743358808157,"user_tz":240,"elapsed":155,"user":{"displayName":"Luopeiwen Yi","userId":"10397336716270013892"}}},"outputs":[],"source":["net = ResNetCIFAR(num_layers=20, Nbits=None)\n","net = net.to(device)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BKmqazq4EuUz","executionInfo":{"status":"ok","timestamp":1743337550485,"user_tz":240,"elapsed":21893,"user":{"displayName":"Luopeiwen Yi","userId":"10397336716270013892"}},"outputId":"33f16a9a-140f-4fe6-a92d-79cbfa0efbc4"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 170M/170M [00:13<00:00, 12.7MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss=0.3231, Test accuracy=0.9151\n"]}],"source":["# Load the best weight paramters\n","net.load_state_dict(torch.load(os.path.join(CODE_PATH, \"pretrained_model.pt\")))\n","test(net)"]},{"cell_type":"markdown","source":["Test accuracy of the floating-point pretrained model:  91.51%"],"metadata":{"id":"DKE77fXOe3-j"}},{"cell_type":"markdown","source":["## Pruning"],"metadata":{"id":"ckVW0BgpFKA_"}},{"cell_type":"markdown","metadata":{"id":"SJOieI-zEuUz"},"source":["### Prune by percentage\n","\n","Pruning a single layer's weights (like FP_Conv or FP_Linear) by:\n","\n","- Taking the absolute value of weights\n","\n","- Finding the q-th percentile (e.g., q=70 → prune bottom 70% smallest-magnitude weights)\n","\n","- Creating a binary mask to zero out those small weights\n","\n"]},{"cell_type":"code","source":["def prune_by_percentage(layer, q=70.0):\n","    \"\"\"\n","    Prune the weight parameters of a layer by zeroing out the\n","    bottom-q percent smallest magnitude weights.\n","    \"\"\"\n","    # Convert weight to numpy array (detach from graph)\n","    weight = layer.weight.data.cpu().numpy()\n","\n","    # Calculate threshold at q-th percentile of absolute weight values\n","    threshold = np.percentile(np.abs(weight), q)\n","\n","    # Create mask: keep weights >= threshold\n","    mask = np.abs(weight) >= threshold\n","\n","    # Convert mask to torch tensor and move to same device as weight\n","    mask_tensor = torch.tensor(mask, dtype=torch.float32, device=layer.weight.device)\n","\n","    # Apply mask (in-place pruning)\n","    layer.weight.data.mul_(mask_tensor)"],"metadata":{"id":"vW-OZuP5gvAt","executionInfo":{"status":"ok","timestamp":1743304406124,"user_tz":240,"elapsed":1,"user":{"displayName":"Luopeiwen Yi","userId":"10397336716270013892"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["qs = [0.2, 0.4, 0.6, 0.7, 0.8]  # pruning ratios\n","\n","for q in qs:\n","    print(f\"\\n--- Pruning q = {q} ---\")\n","    net.load_state_dict(torch.load(os.path.join(CODE_PATH, \"pretrained_model.pt\")))\n","\n","    for name, layer in net.named_modules():\n","        if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n","            # Convert pruning ratio (0.2) to percentile (20.0)\n","            prune_by_percentage(layer, q=q * 100)\n","\n","            np_weight = layer.weight.data.cpu().numpy()\n","            zeros = np.sum(np_weight == 0)\n","            total = np_weight.size\n","            print(f\"Sparsity of {name}: {zeros}/{total} = {zeros/total:.4f}\")\n","\n","    # Evaluate pruned model\n","    test(net)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KLLS4HZ7hp1R","executionInfo":{"status":"ok","timestamp":1743304416908,"user_tz":240,"elapsed":10776,"user":{"displayName":"Luopeiwen Yi","userId":"10397336716270013892"}},"outputId":"44d09f99-eb34-4173-b029-78c17351f4cf"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Pruning q = 0.2 ---\n","Sparsity of head_conv.0.conv: 87/432 = 0.2014\n","Sparsity of body_op.0.conv1.0.conv: 461/2304 = 0.2001\n","Sparsity of body_op.0.conv2.0.conv: 461/2304 = 0.2001\n","Sparsity of body_op.1.conv1.0.conv: 461/2304 = 0.2001\n","Sparsity of body_op.1.conv2.0.conv: 461/2304 = 0.2001\n","Sparsity of body_op.2.conv1.0.conv: 461/2304 = 0.2001\n","Sparsity of body_op.2.conv2.0.conv: 461/2304 = 0.2001\n","Sparsity of body_op.3.conv1.0.conv: 922/4608 = 0.2001\n","Sparsity of body_op.3.conv2.0.conv: 1843/9216 = 0.2000\n","Sparsity of body_op.4.conv1.0.conv: 1843/9216 = 0.2000\n","Sparsity of body_op.4.conv2.0.conv: 1843/9216 = 0.2000\n","Sparsity of body_op.5.conv1.0.conv: 1843/9216 = 0.2000\n","Sparsity of body_op.5.conv2.0.conv: 1843/9216 = 0.2000\n","Sparsity of body_op.6.conv1.0.conv: 3687/18432 = 0.2000\n","Sparsity of body_op.6.conv2.0.conv: 7373/36864 = 0.2000\n","Sparsity of body_op.7.conv1.0.conv: 7373/36864 = 0.2000\n","Sparsity of body_op.7.conv2.0.conv: 7373/36864 = 0.2000\n","Sparsity of body_op.8.conv1.0.conv: 7373/36864 = 0.2000\n","Sparsity of body_op.8.conv2.0.conv: 7373/36864 = 0.2000\n","Sparsity of final_fc.linear: 128/640 = 0.2000\n","Test Loss=0.3450, Test accuracy=0.9087\n","\n","--- Pruning q = 0.4 ---\n","Sparsity of head_conv.0.conv: 173/432 = 0.4005\n","Sparsity of body_op.0.conv1.0.conv: 922/2304 = 0.4002\n","Sparsity of body_op.0.conv2.0.conv: 922/2304 = 0.4002\n","Sparsity of body_op.1.conv1.0.conv: 922/2304 = 0.4002\n","Sparsity of body_op.1.conv2.0.conv: 922/2304 = 0.4002\n","Sparsity of body_op.2.conv1.0.conv: 922/2304 = 0.4002\n","Sparsity of body_op.2.conv2.0.conv: 922/2304 = 0.4002\n","Sparsity of body_op.3.conv1.0.conv: 1843/4608 = 0.4000\n","Sparsity of body_op.3.conv2.0.conv: 3686/9216 = 0.4000\n","Sparsity of body_op.4.conv1.0.conv: 3686/9216 = 0.4000\n","Sparsity of body_op.4.conv2.0.conv: 3686/9216 = 0.4000\n","Sparsity of body_op.5.conv1.0.conv: 3686/9216 = 0.4000\n","Sparsity of body_op.5.conv2.0.conv: 3686/9216 = 0.4000\n","Sparsity of body_op.6.conv1.0.conv: 7373/18432 = 0.4000\n","Sparsity of body_op.6.conv2.0.conv: 14746/36864 = 0.4000\n","Sparsity of body_op.7.conv1.0.conv: 14746/36864 = 0.4000\n","Sparsity of body_op.7.conv2.0.conv: 14746/36864 = 0.4000\n","Sparsity of body_op.8.conv1.0.conv: 14746/36864 = 0.4000\n","Sparsity of body_op.8.conv2.0.conv: 14746/36864 = 0.4000\n","Sparsity of final_fc.linear: 256/640 = 0.4000\n","Test Loss=0.4307, Test accuracy=0.8873\n","\n","--- Pruning q = 0.6 ---\n","Sparsity of head_conv.0.conv: 259/432 = 0.5995\n","Sparsity of body_op.0.conv1.0.conv: 1382/2304 = 0.5998\n","Sparsity of body_op.0.conv2.0.conv: 1382/2304 = 0.5998\n","Sparsity of body_op.1.conv1.0.conv: 1382/2304 = 0.5998\n","Sparsity of body_op.1.conv2.0.conv: 1382/2304 = 0.5998\n","Sparsity of body_op.2.conv1.0.conv: 1382/2304 = 0.5998\n","Sparsity of body_op.2.conv2.0.conv: 1382/2304 = 0.5998\n","Sparsity of body_op.3.conv1.0.conv: 2765/4608 = 0.6000\n","Sparsity of body_op.3.conv2.0.conv: 5529/9216 = 0.5999\n","Sparsity of body_op.4.conv1.0.conv: 5529/9216 = 0.5999\n","Sparsity of body_op.4.conv2.0.conv: 5529/9216 = 0.5999\n","Sparsity of body_op.5.conv1.0.conv: 5529/9216 = 0.5999\n","Sparsity of body_op.5.conv2.0.conv: 5529/9216 = 0.5999\n","Sparsity of body_op.6.conv1.0.conv: 11059/18432 = 0.6000\n","Sparsity of body_op.6.conv2.0.conv: 22118/36864 = 0.6000\n","Sparsity of body_op.7.conv1.0.conv: 22118/36864 = 0.6000\n","Sparsity of body_op.7.conv2.0.conv: 22118/36864 = 0.6000\n","Sparsity of body_op.8.conv1.0.conv: 22118/36864 = 0.6000\n","Sparsity of body_op.8.conv2.0.conv: 22118/36864 = 0.6000\n","Sparsity of final_fc.linear: 384/640 = 0.6000\n","Test Loss=1.0948, Test accuracy=0.7222\n","\n","--- Pruning q = 0.7 ---\n","Sparsity of head_conv.0.conv: 302/432 = 0.6991\n","Sparsity of body_op.0.conv1.0.conv: 1613/2304 = 0.7001\n","Sparsity of body_op.0.conv2.0.conv: 1613/2304 = 0.7001\n","Sparsity of body_op.1.conv1.0.conv: 1613/2304 = 0.7001\n","Sparsity of body_op.1.conv2.0.conv: 1613/2304 = 0.7001\n","Sparsity of body_op.2.conv1.0.conv: 1613/2304 = 0.7001\n","Sparsity of body_op.2.conv2.0.conv: 1613/2304 = 0.7001\n","Sparsity of body_op.3.conv1.0.conv: 3225/4608 = 0.6999\n","Sparsity of body_op.3.conv2.0.conv: 6451/9216 = 0.7000\n","Sparsity of body_op.4.conv1.0.conv: 6451/9216 = 0.7000\n","Sparsity of body_op.4.conv2.0.conv: 6451/9216 = 0.7000\n","Sparsity of body_op.5.conv1.0.conv: 6451/9216 = 0.7000\n","Sparsity of body_op.5.conv2.0.conv: 6451/9216 = 0.7000\n","Sparsity of body_op.6.conv1.0.conv: 12902/18432 = 0.7000\n","Sparsity of body_op.6.conv2.0.conv: 25805/36864 = 0.7000\n","Sparsity of body_op.7.conv1.0.conv: 25805/36864 = 0.7000\n","Sparsity of body_op.7.conv2.0.conv: 25805/36864 = 0.7000\n","Sparsity of body_op.8.conv1.0.conv: 25805/36864 = 0.7000\n","Sparsity of body_op.8.conv2.0.conv: 25805/36864 = 0.7000\n","Sparsity of final_fc.linear: 448/640 = 0.7000\n","Test Loss=2.4419, Test accuracy=0.4204\n","\n","--- Pruning q = 0.8 ---\n","Sparsity of head_conv.0.conv: 345/432 = 0.7986\n","Sparsity of body_op.0.conv1.0.conv: 1843/2304 = 0.7999\n","Sparsity of body_op.0.conv2.0.conv: 1843/2304 = 0.7999\n","Sparsity of body_op.1.conv1.0.conv: 1843/2304 = 0.7999\n","Sparsity of body_op.1.conv2.0.conv: 1843/2304 = 0.7999\n","Sparsity of body_op.2.conv1.0.conv: 1843/2304 = 0.7999\n","Sparsity of body_op.2.conv2.0.conv: 1843/2304 = 0.7999\n","Sparsity of body_op.3.conv1.0.conv: 3686/4608 = 0.7999\n","Sparsity of body_op.3.conv2.0.conv: 7372/9216 = 0.7999\n","Sparsity of body_op.4.conv1.0.conv: 7372/9216 = 0.7999\n","Sparsity of body_op.4.conv2.0.conv: 7372/9216 = 0.7999\n","Sparsity of body_op.5.conv1.0.conv: 7372/9216 = 0.7999\n","Sparsity of body_op.5.conv2.0.conv: 7372/9216 = 0.7999\n","Sparsity of body_op.6.conv1.0.conv: 14745/18432 = 0.8000\n","Sparsity of body_op.6.conv2.0.conv: 29491/36864 = 0.8000\n","Sparsity of body_op.7.conv1.0.conv: 29491/36864 = 0.8000\n","Sparsity of body_op.7.conv2.0.conv: 29491/36864 = 0.8000\n","Sparsity of body_op.8.conv1.0.conv: 29491/36864 = 0.8000\n","Sparsity of body_op.8.conv2.0.conv: 29491/36864 = 0.8000\n","Sparsity of final_fc.linear: 512/640 = 0.8000\n","Test Loss=6.3135, Test accuracy=0.1003\n"]}]},{"cell_type":"markdown","source":["### Finetune pruned model"],"metadata":{"id":"1ooYU4ncFVzt"}},{"cell_type":"markdown","metadata":{"id":"4VVEXD6kEuU0"},"source":["#### One-Shot Pruning + Fine-Tuning\n","\n","Methodology:\n","\n","- Prune once: Remove a large percentage of weights (e.g., 80%) in one go.\n","\n","- Fix the sparsity mask: Ensure pruned weights are permanently zero.\n","\n","- Fine-tune the model with this fixed sparsity structure for multiple epochs to recover performance.\n","\n","Intuition:\n","\n","- This is a brute-force strategy.\n","\n","- The model suddenly loses a large portion of its parameters.\n","\n","- It then tries to re-adapt using only the remaining weights.\n","\n","- Downside: Pruning so many weights all at once may cause irreversible accuracy loss, especially if some important weights are accidentally removed.\n","\n"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"sw8UJ4QAEuU0","executionInfo":{"status":"ok","timestamp":1743304416913,"user_tz":240,"elapsed":2,"user":{"displayName":"Luopeiwen Yi","userId":"10397336716270013892"}}},"outputs":[],"source":["def finetune_after_prune(net, trainloader, criterion, optimizer, prune=True):\n","    \"\"\"\n","    Finetune the pruned model for a single epoch.\n","    Ensures pruned weights remain zero throughout training.\n","    \"\"\"\n","    weight_mask = {}\n","    for name, layer in net.named_modules():\n","        if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n","            mask = (layer.weight.data != 0).float().to(layer.weight.device)\n","            weight_mask[name] = mask\n","\n","    global_steps = 0\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","    start = time.time()\n","\n","    for batch_idx, (inputs, targets) in enumerate(trainloader):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Keep pruned weights at 0\n","        if prune:\n","            for name, layer in net.named_modules():\n","                if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n","                    layer.weight.data.mul_(weight_mask[name])  # apply mask again\n","\n","        train_loss += loss.item()\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","        global_steps += 1\n","\n","        if global_steps % 50 == 0:\n","            end = time.time()\n","            batch_size = 256\n","            num_examples_per_second = 50 * batch_size / (end - start)\n","            print(\"[Step=%d]\\tLoss=%.4f\\tacc=%.4f\\t%.1f examples/second\"\n","                 % (global_steps, train_loss / (batch_idx + 1), (correct / total), num_examples_per_second))\n","            start = time.time()"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NO-905m1EuU0","executionInfo":{"status":"ok","timestamp":1743304418594,"user_tz":240,"elapsed":1681,"user":{"displayName":"Luopeiwen Yi","userId":"10397336716270013892"}},"outputId":"0ef09d81-f1df-4d73-fc74-9c53faa54f72"},"outputs":[{"output_type":"stream","name":"stdout","text":["==> Preparing data..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]}],"source":["# Get pruned model\n","net.load_state_dict(torch.load(os.path.join(CODE_PATH, \"pretrained_model.pt\")))\n","for name,layer in net.named_modules():\n","    if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n","        prune_by_percentage(layer, q=80.0)\n","\n","# Training setup, do not change\n","batch_size=256\n","lr=0.002\n","reg=1e-4\n","\n","print('==> Preparing data..')\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","best_acc = 0  # best test accuracy\n","start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=16)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.875, weight_decay=reg, nesterov=False)"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WaFVnDyHEuU0","executionInfo":{"status":"ok","timestamp":1743304558245,"user_tz":240,"elapsed":139649,"user":{"displayName":"Luopeiwen Yi","userId":"10397336716270013892"}},"outputId":"337a5c9c-8560-4434-fc1d-272ac93fe5b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Epoch: 0\n","[Step=50]\tLoss=0.9780\tacc=0.6847\t6640.2 examples/second\n","[Step=100]\tLoss=0.8098\tacc=0.7359\t9687.6 examples/second\n","[Step=150]\tLoss=0.7194\tacc=0.7633\t9641.5 examples/second\n","Test Loss=0.5586, Test acc=0.8190\n","Saving...\n","\n","Epoch: 1\n","[Step=50]\tLoss=0.4706\tacc=0.8413\t6606.8 examples/second\n","[Step=100]\tLoss=0.4605\tacc=0.8437\t10447.3 examples/second\n","[Step=150]\tLoss=0.4504\tacc=0.8470\t10463.5 examples/second\n","Test Loss=0.4865, Test acc=0.8383\n","Saving...\n","\n","Epoch: 2\n","[Step=50]\tLoss=0.4058\tacc=0.8622\t6693.9 examples/second\n","[Step=100]\tLoss=0.4013\tacc=0.8627\t9733.1 examples/second\n","[Step=150]\tLoss=0.3965\tacc=0.8645\t10157.9 examples/second\n","Test Loss=0.4539, Test acc=0.8495\n","Saving...\n","\n","Epoch: 3\n","[Step=50]\tLoss=0.3748\tacc=0.8708\t6856.6 examples/second\n","[Step=100]\tLoss=0.3741\tacc=0.8712\t10171.4 examples/second\n","[Step=150]\tLoss=0.3651\tacc=0.8751\t10560.8 examples/second\n","Test Loss=0.4343, Test acc=0.8568\n","Saving...\n","\n","Epoch: 4\n","[Step=50]\tLoss=0.3523\tacc=0.8836\t6387.7 examples/second\n","[Step=100]\tLoss=0.3455\tacc=0.8835\t10427.9 examples/second\n","[Step=150]\tLoss=0.3410\tacc=0.8846\t10465.1 examples/second\n","Test Loss=0.4200, Test acc=0.8608\n","Saving...\n","\n","Epoch: 5\n","[Step=50]\tLoss=0.3354\tacc=0.8852\t6449.3 examples/second\n","[Step=100]\tLoss=0.3296\tacc=0.8871\t10337.1 examples/second\n","[Step=150]\tLoss=0.3295\tacc=0.8872\t10301.8 examples/second\n","Test Loss=0.4125, Test acc=0.8638\n","Saving...\n","\n","Epoch: 6\n","[Step=50]\tLoss=0.3067\tacc=0.8915\t6688.2 examples/second\n","[Step=100]\tLoss=0.3091\tacc=0.8921\t10644.2 examples/second\n","[Step=150]\tLoss=0.3128\tacc=0.8921\t10923.9 examples/second\n","Test Loss=0.4040, Test acc=0.8664\n","Saving...\n","\n","Epoch: 7\n","[Step=50]\tLoss=0.3073\tacc=0.8955\t6761.1 examples/second\n","[Step=100]\tLoss=0.3007\tacc=0.8973\t10752.3 examples/second\n","[Step=150]\tLoss=0.3017\tacc=0.8967\t9963.1 examples/second\n","Test Loss=0.3979, Test acc=0.8678\n","Saving...\n","\n","Epoch: 8\n","[Step=50]\tLoss=0.2991\tacc=0.8984\t6731.3 examples/second\n","[Step=100]\tLoss=0.2954\tacc=0.8992\t10195.3 examples/second\n","[Step=150]\tLoss=0.2931\tacc=0.8994\t10441.2 examples/second\n","Test Loss=0.3916, Test acc=0.8693\n","Saving...\n","\n","Epoch: 9\n","[Step=50]\tLoss=0.2838\tacc=0.9031\t6757.1 examples/second\n","[Step=100]\tLoss=0.2913\tacc=0.9007\t9773.7 examples/second\n","[Step=150]\tLoss=0.2890\tacc=0.9004\t9665.1 examples/second\n","Test Loss=0.3885, Test acc=0.8697\n","Saving...\n","\n","Epoch: 10\n","[Step=50]\tLoss=0.2788\tacc=0.9009\t6762.3 examples/second\n","[Step=100]\tLoss=0.2808\tacc=0.9018\t10354.3 examples/second\n","[Step=150]\tLoss=0.2815\tacc=0.9013\t10305.2 examples/second\n","Test Loss=0.3870, Test acc=0.8703\n","Saving...\n","\n","Epoch: 11\n","[Step=50]\tLoss=0.2778\tacc=0.9030\t6452.6 examples/second\n","[Step=100]\tLoss=0.2737\tacc=0.9047\t9433.6 examples/second\n","[Step=150]\tLoss=0.2758\tacc=0.9038\t10098.7 examples/second\n","Test Loss=0.3814, Test acc=0.8735\n","Saving...\n","\n","Epoch: 12\n","[Step=50]\tLoss=0.2716\tacc=0.9049\t6660.5 examples/second\n","[Step=100]\tLoss=0.2736\tacc=0.9054\t10351.8 examples/second\n","[Step=150]\tLoss=0.2697\tacc=0.9065\t10322.2 examples/second\n","Test Loss=0.3786, Test acc=0.8734\n","\n","Epoch: 13\n","[Step=50]\tLoss=0.2632\tacc=0.9097\t6482.0 examples/second\n","[Step=100]\tLoss=0.2616\tacc=0.9079\t10196.2 examples/second\n","[Step=150]\tLoss=0.2661\tacc=0.9070\t10079.1 examples/second\n","Test Loss=0.3753, Test acc=0.8758\n","Saving...\n","\n","Epoch: 14\n","[Step=50]\tLoss=0.2527\tacc=0.9149\t6755.5 examples/second\n","[Step=100]\tLoss=0.2561\tacc=0.9118\t9745.6 examples/second\n","[Step=150]\tLoss=0.2581\tacc=0.9111\t9795.2 examples/second\n","Test Loss=0.3732, Test acc=0.8756\n","\n","Epoch: 15\n","[Step=50]\tLoss=0.2495\tacc=0.9142\t6891.8 examples/second\n","[Step=100]\tLoss=0.2551\tacc=0.9126\t10400.3 examples/second\n","[Step=150]\tLoss=0.2544\tacc=0.9124\t10244.3 examples/second\n","Test Loss=0.3703, Test acc=0.8773\n","Saving...\n","\n","Epoch: 16\n","[Step=50]\tLoss=0.2572\tacc=0.9101\t6771.4 examples/second\n","[Step=100]\tLoss=0.2534\tacc=0.9122\t9885.9 examples/second\n","[Step=150]\tLoss=0.2528\tacc=0.9125\t9836.8 examples/second\n","Test Loss=0.3695, Test acc=0.8775\n","Saving...\n","\n","Epoch: 17\n","[Step=50]\tLoss=0.2506\tacc=0.9141\t6654.8 examples/second\n","[Step=100]\tLoss=0.2497\tacc=0.9153\t10490.5 examples/second\n","[Step=150]\tLoss=0.2486\tacc=0.9151\t10164.3 examples/second\n","Test Loss=0.3676, Test acc=0.8784\n","Saving...\n","\n","Epoch: 18\n","[Step=50]\tLoss=0.2415\tacc=0.9182\t6404.0 examples/second\n","[Step=100]\tLoss=0.2417\tacc=0.9167\t9550.2 examples/second\n","[Step=150]\tLoss=0.2428\tacc=0.9156\t9919.8 examples/second\n","Test Loss=0.3664, Test acc=0.8794\n","Saving...\n","\n","Epoch: 19\n","[Step=50]\tLoss=0.2419\tacc=0.9173\t6710.7 examples/second\n","[Step=100]\tLoss=0.2426\tacc=0.9175\t10116.3 examples/second\n","[Step=150]\tLoss=0.2418\tacc=0.9173\t10147.1 examples/second\n","Test Loss=0.3663, Test acc=0.8792\n"]}],"source":["# Model finetuning\n","for epoch in range(20):\n","    print('\\nEpoch: %d' % epoch)\n","    net.train()\n","    finetune_after_prune(net, trainloader, criterion, optimizer)\n","    #Start the testing code.\n","    net.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(testloader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = net(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            test_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","    num_val_steps = len(testloader)\n","    val_acc = correct / total\n","    print(\"Test Loss=%.4f, Test acc=%.4f\" % (test_loss / (num_val_steps), val_acc))\n","\n","    if val_acc > best_acc:\n","        best_acc = val_acc\n","        print(\"Saving...\")\n","        torch.save(net.state_dict(), os.path.join(CODE_PATH, \"net_after_finetune.pt\"))"]},{"cell_type":"code","source":["net.load_state_dict(torch.load(os.path.join(CODE_PATH, \"net_after_finetune.pt\")))\n","\n","for name, layer in net.named_modules():\n","    if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n","        np_weight = layer.weight.data.cpu().numpy()\n","        zeros = np.sum(np_weight == 0)\n","        total = np_weight.size\n","        print('Sparsity of '+name+': '+str(zeros / total))\n","\n","test(net)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LjZkZmDZmdif","executionInfo":{"status":"ok","timestamp":1743304560240,"user_tz":240,"elapsed":1973,"user":{"displayName":"Luopeiwen Yi","userId":"10397336716270013892"}},"outputId":"46dc7660-0b5c-48db-dd1b-3284cb26d6ac"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Sparsity of head_conv.0.conv: 0.7986111111111112\n","Sparsity of body_op.0.conv1.0.conv: 0.7999131944444444\n","Sparsity of body_op.0.conv2.0.conv: 0.7999131944444444\n","Sparsity of body_op.1.conv1.0.conv: 0.7999131944444444\n","Sparsity of body_op.1.conv2.0.conv: 0.7999131944444444\n","Sparsity of body_op.2.conv1.0.conv: 0.7999131944444444\n","Sparsity of body_op.2.conv2.0.conv: 0.7999131944444444\n","Sparsity of body_op.3.conv1.0.conv: 0.7999131944444444\n","Sparsity of body_op.3.conv2.0.conv: 0.7999131944444444\n","Sparsity of body_op.4.conv1.0.conv: 0.7999131944444444\n","Sparsity of body_op.4.conv2.0.conv: 0.7999131944444444\n","Sparsity of body_op.5.conv1.0.conv: 0.7999131944444444\n","Sparsity of body_op.5.conv2.0.conv: 0.7999131944444444\n","Sparsity of body_op.6.conv1.0.conv: 0.7999674479166666\n","Sparsity of body_op.6.conv2.0.conv: 0.7999945746527778\n","Sparsity of body_op.7.conv1.0.conv: 0.7999945746527778\n","Sparsity of body_op.7.conv2.0.conv: 0.7999945746527778\n","Sparsity of body_op.8.conv1.0.conv: 0.7999945746527778\n","Sparsity of body_op.8.conv2.0.conv: 0.7999945746527778\n","Sparsity of final_fc.linear: 0.8\n","Test Loss=0.3664, Test accuracy=0.8794\n"]}]},{"cell_type":"markdown","source":["Test Accuracy of One-Shot Pruning + Fine-tuning at 80% sparsity level: 87.94%"],"metadata":{"id":"R8RHhjT1D4aN"}},{"cell_type":"markdown","metadata":{"id":"XdvCoRg_EuU1"},"source":["#### Iterative Pruning + Fine-tuning\n","\n","Methodology:\n","\n","- Start from a dense (unpruned) pretrained model.\n","\n","- For each of the first 10 epochs:\n","\n","  - Gradually increase pruning percentage, e.g., prune 8%, 16%, ..., up to 80%.\n","\n","  - Allow the network to adjust (fine-tune) after each pruning step, and even recover pruned weights.\n","  \n","  - After reaching 80% pruning at epoch 10, freeze the zero weights and continue fine-tuning with fixed sparsity for the next 10 epochs.\n","\n","Intuition:\n","\n","- This is a progressive or gradual approach.\n","\n","- The model has a chance to redistribute importance among weights during early training.\n","\n","- Weights that get pruned early can still come back (until sparsity is fixed).\n","\n","- This encourages the network to naturally evolve toward sparse representations.\n","\n","- Generally results in better performance and stability, especially under high sparsity levels."]},{"cell_type":"code","source":["net.load_state_dict(torch.load(os.path.join(CODE_PATH, \"pretrained_model.pt\")))\n","best_acc = 0.\n","\n","for epoch in range(20):\n","    print('\\nEpoch: %d' % epoch)\n","    net.train()\n","\n","    if epoch < 10:\n","      q = 8 * (epoch + 1)\n","      print(f\"Pruning {q:.1f}% of weights before Epoch {epoch}\")\n","\n","      for name, layer in net.named_modules():\n","          if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n","              prune_by_percentage(layer, q=q)\n","\n","    if epoch < 9:\n","        finetune_after_prune(net, trainloader, criterion, optimizer, prune=False)  # pruning not enforced yet\n","    else:\n","        finetune_after_prune(net, trainloader, criterion, optimizer, prune=True)   # enforce pruned weights remain 0\n","\n","    # Evaluation\n","    net.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(testloader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = net(inputs)\n","            loss = criterion(outputs, targets)\n","            test_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","    num_val_steps = len(testloader)\n","    val_acc = correct / total\n","    print(\"Test Loss = %.4f, Test acc = %.4f\" % (test_loss / num_val_steps, val_acc))\n","\n","    if epoch >= 10 and val_acc > best_acc:\n","        best_acc = val_acc\n","        print(\"Saving...\")\n","        torch.save(net.state_dict(), os.path.join(CODE_PATH, \"net_after_iterative_prune.pt\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M6rKrPHQsp7g","executionInfo":{"status":"ok","timestamp":1743304701084,"user_tz":240,"elapsed":140841,"user":{"displayName":"Luopeiwen Yi","userId":"10397336716270013892"}},"outputId":"0fd8c012-75b4-45f5-9acb-69a6041b6a5b"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Epoch: 0\n","Pruning 8.0% of weights before Epoch 0\n","[Step=50]\tLoss=0.0465\tacc=0.9862\t6832.9 examples/second\n","[Step=100]\tLoss=0.0474\tacc=0.9853\t10718.8 examples/second\n","[Step=150]\tLoss=0.0479\tacc=0.9844\t10429.4 examples/second\n","Test Loss = 0.3248, Test acc = 0.9129\n","\n","Epoch: 1\n","Pruning 16.0% of weights before Epoch 1\n","[Step=50]\tLoss=0.0468\tacc=0.9844\t6686.1 examples/second\n","[Step=100]\tLoss=0.0476\tacc=0.9846\t9990.2 examples/second\n","[Step=150]\tLoss=0.0479\tacc=0.9844\t9537.6 examples/second\n","Test Loss = 0.3275, Test acc = 0.9149\n","\n","Epoch: 2\n","Pruning 24.0% of weights before Epoch 2\n","[Step=50]\tLoss=0.0522\tacc=0.9842\t6586.6 examples/second\n","[Step=100]\tLoss=0.0530\tacc=0.9838\t9894.4 examples/second\n","[Step=150]\tLoss=0.0526\tacc=0.9836\t10124.1 examples/second\n","Test Loss = 0.3273, Test acc = 0.9131\n","\n","Epoch: 3\n","Pruning 32.0% of weights before Epoch 3\n","[Step=50]\tLoss=0.0571\tacc=0.9816\t6578.9 examples/second\n","[Step=100]\tLoss=0.0576\tacc=0.9808\t9579.8 examples/second\n","[Step=150]\tLoss=0.0567\tacc=0.9813\t10278.6 examples/second\n","Test Loss = 0.3327, Test acc = 0.9114\n","\n","Epoch: 4\n","Pruning 40.0% of weights before Epoch 4\n","[Step=50]\tLoss=0.0664\tacc=0.9774\t6503.8 examples/second\n","[Step=100]\tLoss=0.0660\tacc=0.9783\t10460.9 examples/second\n","[Step=150]\tLoss=0.0678\tacc=0.9773\t10462.5 examples/second\n","Test Loss = 0.3379, Test acc = 0.9103\n","\n","Epoch: 5\n","Pruning 48.0% of weights before Epoch 5\n","[Step=50]\tLoss=0.0906\tacc=0.9696\t6498.9 examples/second\n","[Step=100]\tLoss=0.0908\tacc=0.9687\t10024.2 examples/second\n","[Step=150]\tLoss=0.0875\tacc=0.9699\t10306.1 examples/second\n","Test Loss = 0.3356, Test acc = 0.9064\n","\n","Epoch: 6\n","Pruning 56.0% of weights before Epoch 6\n","[Step=50]\tLoss=0.1327\tacc=0.9524\t5293.5 examples/second\n","[Step=100]\tLoss=0.1276\tacc=0.9546\t10140.8 examples/second\n","[Step=150]\tLoss=0.1209\tacc=0.9574\t10334.8 examples/second\n","Test Loss = 0.3383, Test acc = 0.9015\n","\n","Epoch: 7\n","Pruning 64.0% of weights before Epoch 7\n","[Step=50]\tLoss=0.1788\tacc=0.9380\t6564.8 examples/second\n","[Step=100]\tLoss=0.1730\tacc=0.9390\t10219.3 examples/second\n","[Step=150]\tLoss=0.1676\tacc=0.9412\t10425.1 examples/second\n","Test Loss = 0.3495, Test acc = 0.8928\n","\n","Epoch: 8\n","Pruning 72.0% of weights before Epoch 8\n","[Step=50]\tLoss=0.2923\tacc=0.8957\t6574.7 examples/second\n","[Step=100]\tLoss=0.2654\tacc=0.9057\t9842.6 examples/second\n","[Step=150]\tLoss=0.2512\tacc=0.9119\t9641.0 examples/second\n","Test Loss = 0.3687, Test acc = 0.8818\n","\n","Epoch: 9\n","Pruning 80.0% of weights before Epoch 9\n","[Step=50]\tLoss=0.5638\tacc=0.8070\t6519.0 examples/second\n","[Step=100]\tLoss=0.5079\tacc=0.8266\t10293.8 examples/second\n","[Step=150]\tLoss=0.4751\tacc=0.8389\t9966.2 examples/second\n","Test Loss = 0.4725, Test acc = 0.8484\n","\n","Epoch: 10\n","[Step=50]\tLoss=0.3744\tacc=0.8723\t6511.2 examples/second\n","[Step=100]\tLoss=0.3726\tacc=0.8732\t9498.4 examples/second\n","[Step=150]\tLoss=0.3694\tacc=0.8748\t9925.0 examples/second\n","Test Loss = 0.4402, Test acc = 0.8574\n","Saving...\n","\n","Epoch: 11\n","[Step=50]\tLoss=0.3423\tacc=0.8846\t6776.0 examples/second\n","[Step=100]\tLoss=0.3418\tacc=0.8845\t10506.7 examples/second\n","[Step=150]\tLoss=0.3406\tacc=0.8839\t10116.4 examples/second\n","Test Loss = 0.4228, Test acc = 0.8625\n","Saving...\n","\n","Epoch: 12\n","[Step=50]\tLoss=0.3317\tacc=0.8855\t6522.4 examples/second\n","[Step=100]\tLoss=0.3272\tacc=0.8877\t10153.8 examples/second\n","[Step=150]\tLoss=0.3226\tacc=0.8898\t9957.6 examples/second\n","Test Loss = 0.4105, Test acc = 0.8660\n","Saving...\n","\n","Epoch: 13\n","[Step=50]\tLoss=0.3085\tacc=0.8959\t6655.7 examples/second\n","[Step=100]\tLoss=0.3028\tacc=0.8969\t10158.2 examples/second\n","[Step=150]\tLoss=0.3012\tacc=0.8971\t9966.2 examples/second\n","Test Loss = 0.4025, Test acc = 0.8693\n","Saving...\n","\n","Epoch: 14\n","[Step=50]\tLoss=0.3039\tacc=0.8946\t6703.7 examples/second\n","[Step=100]\tLoss=0.2983\tacc=0.8973\t10591.6 examples/second\n","[Step=150]\tLoss=0.2959\tacc=0.8986\t10586.8 examples/second\n","Test Loss = 0.3964, Test acc = 0.8705\n","Saving...\n","\n","Epoch: 15\n","[Step=50]\tLoss=0.2861\tacc=0.9028\t6607.9 examples/second\n","[Step=100]\tLoss=0.2882\tacc=0.9007\t9553.3 examples/second\n","[Step=150]\tLoss=0.2853\tacc=0.9028\t9850.4 examples/second\n","Test Loss = 0.3890, Test acc = 0.8725\n","Saving...\n","\n","Epoch: 16\n","[Step=50]\tLoss=0.2777\tacc=0.9048\t6567.8 examples/second\n","[Step=100]\tLoss=0.2760\tacc=0.9060\t10489.9 examples/second\n","[Step=150]\tLoss=0.2735\tacc=0.9066\t10065.5 examples/second\n","Test Loss = 0.3858, Test acc = 0.8724\n","\n","Epoch: 17\n","[Step=50]\tLoss=0.2797\tacc=0.9001\t6537.6 examples/second\n","[Step=100]\tLoss=0.2755\tacc=0.9026\t10249.5 examples/second\n","[Step=150]\tLoss=0.2760\tacc=0.9035\t9814.6 examples/second\n","Test Loss = 0.3821, Test acc = 0.8741\n","Saving...\n","\n","Epoch: 18\n","[Step=50]\tLoss=0.2634\tacc=0.9095\t6579.4 examples/second\n","[Step=100]\tLoss=0.2667\tacc=0.9077\t10447.4 examples/second\n","[Step=150]\tLoss=0.2662\tacc=0.9076\t10433.6 examples/second\n","Test Loss = 0.3784, Test acc = 0.8761\n","Saving...\n","\n","Epoch: 19\n","[Step=50]\tLoss=0.2567\tacc=0.9102\t6338.1 examples/second\n","[Step=100]\tLoss=0.2550\tacc=0.9112\t10471.7 examples/second\n","[Step=150]\tLoss=0.2572\tacc=0.9109\t10088.4 examples/second\n","Test Loss = 0.3750, Test acc = 0.8769\n","Saving...\n"]}]},{"cell_type":"code","source":["net.load_state_dict(torch.load(os.path.join(CODE_PATH, \"net_after_iterative_prune.pt\")))\n","\n","for name, layer in net.named_modules():\n","    if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n","        np_weight = layer.weight.data.cpu().numpy()\n","        zeros = np.sum(np_weight == 0)\n","        total = np_weight.size\n","        print(f\"Sparsity of {name}: {zeros}/{total} = {zeros/total:.4f}\")\n","\n","# Final test\n","test(net)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i-me1ymQtUfV","executionInfo":{"status":"ok","timestamp":1743304703223,"user_tz":240,"elapsed":2122,"user":{"displayName":"Luopeiwen Yi","userId":"10397336716270013892"}},"outputId":"98383045-d070-4019-bb64-46d81e6d6fee"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Sparsity of head_conv.0.conv: 345/432 = 0.7986\n","Sparsity of body_op.0.conv1.0.conv: 1843/2304 = 0.7999\n","Sparsity of body_op.0.conv2.0.conv: 1843/2304 = 0.7999\n","Sparsity of body_op.1.conv1.0.conv: 1843/2304 = 0.7999\n","Sparsity of body_op.1.conv2.0.conv: 1843/2304 = 0.7999\n","Sparsity of body_op.2.conv1.0.conv: 1843/2304 = 0.7999\n","Sparsity of body_op.2.conv2.0.conv: 1843/2304 = 0.7999\n","Sparsity of body_op.3.conv1.0.conv: 3686/4608 = 0.7999\n","Sparsity of body_op.3.conv2.0.conv: 7372/9216 = 0.7999\n","Sparsity of body_op.4.conv1.0.conv: 7372/9216 = 0.7999\n","Sparsity of body_op.4.conv2.0.conv: 7372/9216 = 0.7999\n","Sparsity of body_op.5.conv1.0.conv: 7372/9216 = 0.7999\n","Sparsity of body_op.5.conv2.0.conv: 7372/9216 = 0.7999\n","Sparsity of body_op.6.conv1.0.conv: 14745/18432 = 0.8000\n","Sparsity of body_op.6.conv2.0.conv: 29491/36864 = 0.8000\n","Sparsity of body_op.7.conv1.0.conv: 29491/36864 = 0.8000\n","Sparsity of body_op.7.conv2.0.conv: 29491/36864 = 0.8000\n","Sparsity of body_op.8.conv1.0.conv: 29491/36864 = 0.8000\n","Sparsity of body_op.8.conv2.0.conv: 29491/36864 = 0.8000\n","Sparsity of final_fc.linear: 512/640 = 0.8000\n","Test Loss=0.3750, Test accuracy=0.8769\n"]}]},{"cell_type":"markdown","source":["Test Accuracy of Iterative Pruning + Fine-tuning at 80% sparsity level: 87.69%"],"metadata":{"id":"r_jbVminELXX"}},{"cell_type":"markdown","metadata":{"id":"xmEWsQMMEuU1"},"source":["#### Global iterative pruning + Fine-tuning\n","\n","Methodology\n","\n","- Instead of computing thresholds per layer, we:\n","\n","  - Collect all weights across all layers.\n","\n","  - Compute the global q-th percentile threshold.\n","\n","  - Apply this same threshold to all layers — regardless of their own distribution.\n","\n","Intuition\n","\n","- We assume that importance of weights should be measured across the whole model, not just locally in each layer.\n","\n","- Some layers may end up more sparse than others, depending on their weight distributions.\n","\n","Tradeoffs\n","\n","- More flexible: allows sensitive layers to retain more weights.\n","\n","- Potentially better accuracy at same sparsity budget.\n","\n","- Slightly more complex to implement and tune."]},{"cell_type":"code","source":["def global_prune_by_percentage(net, q=70.0):\n","    \"\"\"\n","    Perform global pruning by thresholding weights globally across layers.\n","    :param q: percentile value, e.g., 70.0 means prune bottom 70% smallest-magnitude weights.\n","    \"\"\"\n","    flattened_weights = []\n","\n","    # Gather all weights from prunable layers\n","    for name, layer in net.named_modules():\n","        if isinstance(layer, (nn.Conv2d, nn.Linear)) and 'id_mapping' not in name:\n","            flattened_weights.append(np.abs(layer.weight.data.cpu().numpy()).flatten())\n","\n","    # Compute global threshold\n","    all_weights = np.concatenate(flattened_weights)\n","    threshold = np.percentile(all_weights, q)\n","\n","    # Apply mask globally to each layer\n","    for name, layer in net.named_modules():\n","        if isinstance(layer, (nn.Conv2d, nn.Linear)) and 'id_mapping' not in name:\n","            weight_np = layer.weight.data.cpu().numpy()\n","            mask = np.abs(weight_np) >= threshold\n","            mask_tensor = torch.tensor(mask, dtype=torch.float32, device=layer.weight.device)\n","            layer.weight.data.mul_(mask_tensor)"],"metadata":{"id":"046JqdOU-bNz","executionInfo":{"status":"ok","timestamp":1743304703249,"user_tz":240,"elapsed":30,"user":{"displayName":"Luopeiwen Yi","userId":"10397336716270013892"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yrlK1V-tEuU1","executionInfo":{"status":"ok","timestamp":1743304844400,"user_tz":240,"elapsed":141169,"user":{"displayName":"Luopeiwen Yi","userId":"10397336716270013892"}},"outputId":"ab3e8656-d1de-4dcc-bced-48d4554f068e"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Epoch: 0\n","Global Pruning 8.0% of weights before Epoch 0\n","[Step=50]\tLoss=0.0430\tacc=0.9872\t6701.8 examples/second\n","[Step=100]\tLoss=0.0476\tacc=0.9848\t10210.5 examples/second\n","[Step=150]\tLoss=0.0481\tacc=0.9844\t9593.2 examples/second\n","Test Loss=0.3232, Test acc=0.9151\n","\n","Epoch: 1\n","Global Pruning 16.0% of weights before Epoch 1\n","[Step=50]\tLoss=0.0465\tacc=0.9854\t6708.4 examples/second\n","[Step=100]\tLoss=0.0478\tacc=0.9849\t10075.0 examples/second\n","[Step=150]\tLoss=0.0492\tacc=0.9839\t9992.1 examples/second\n","Test Loss=0.3241, Test acc=0.9155\n","\n","Epoch: 2\n","Global Pruning 24.0% of weights before Epoch 2\n","[Step=50]\tLoss=0.0523\tacc=0.9825\t6739.3 examples/second\n","[Step=100]\tLoss=0.0510\tacc=0.9832\t10000.3 examples/second\n","[Step=150]\tLoss=0.0501\tacc=0.9838\t9596.6 examples/second\n","Test Loss=0.3289, Test acc=0.9139\n","\n","Epoch: 3\n","Global Pruning 32.0% of weights before Epoch 3\n","[Step=50]\tLoss=0.0555\tacc=0.9816\t6694.5 examples/second\n","[Step=100]\tLoss=0.0539\tacc=0.9825\t10436.5 examples/second\n","[Step=150]\tLoss=0.0539\tacc=0.9826\t10438.6 examples/second\n","Test Loss=0.3271, Test acc=0.9143\n","\n","Epoch: 4\n","Global Pruning 40.0% of weights before Epoch 4\n","[Step=50]\tLoss=0.0623\tacc=0.9795\t6620.7 examples/second\n","[Step=100]\tLoss=0.0612\tacc=0.9798\t9641.8 examples/second\n","[Step=150]\tLoss=0.0604\tacc=0.9799\t9972.9 examples/second\n","Test Loss=0.3236, Test acc=0.9137\n","\n","Epoch: 5\n","Global Pruning 48.0% of weights before Epoch 5\n","[Step=50]\tLoss=0.0710\tacc=0.9764\t6786.4 examples/second\n","[Step=100]\tLoss=0.0721\tacc=0.9758\t10711.5 examples/second\n","[Step=150]\tLoss=0.0698\tacc=0.9768\t10057.7 examples/second\n","Test Loss=0.3284, Test acc=0.9078\n","\n","Epoch: 6\n","Global Pruning 56.0% of weights before Epoch 6\n","[Step=50]\tLoss=0.0900\tacc=0.9695\t6378.2 examples/second\n","[Step=100]\tLoss=0.0913\tacc=0.9692\t9828.9 examples/second\n","[Step=150]\tLoss=0.0902\tacc=0.9700\t10105.6 examples/second\n","Test Loss=0.3272, Test acc=0.9059\n","\n","Epoch: 7\n","Global Pruning 64.0% of weights before Epoch 7\n","[Step=50]\tLoss=0.1306\tacc=0.9555\t6546.1 examples/second\n","[Step=100]\tLoss=0.1270\tacc=0.9559\t10047.9 examples/second\n","[Step=150]\tLoss=0.1258\tacc=0.9568\t9697.4 examples/second\n","Test Loss=0.3285, Test acc=0.9018\n","\n","Epoch: 8\n","Global Pruning 72.0% of weights before Epoch 8\n","[Step=50]\tLoss=0.2117\tacc=0.9260\t6522.1 examples/second\n","[Step=100]\tLoss=0.1954\tacc=0.9315\t10274.6 examples/second\n","[Step=150]\tLoss=0.1881\tacc=0.9338\t10185.6 examples/second\n","Test Loss=0.3344, Test acc=0.8883\n","\n","Epoch: 9\n","Global Pruning 80.0% of weights before Epoch 9\n","[Step=50]\tLoss=0.4161\tacc=0.8605\t6775.7 examples/second\n","[Step=100]\tLoss=0.3909\tacc=0.8693\t9961.9 examples/second\n","[Step=150]\tLoss=0.3717\tacc=0.8752\t9686.0 examples/second\n","Test Loss=0.4217, Test acc=0.8572\n","\n","Epoch: 10\n","[Step=50]\tLoss=0.3134\tacc=0.8938\t6477.0 examples/second\n","[Step=100]\tLoss=0.3099\tacc=0.8943\t10405.1 examples/second\n","[Step=150]\tLoss=0.3067\tacc=0.8962\t10128.3 examples/second\n","Test Loss=0.3962, Test acc=0.8655\n","Saving...\n","\n","Epoch: 11\n","[Step=50]\tLoss=0.2852\tacc=0.9062\t6465.1 examples/second\n","[Step=100]\tLoss=0.2883\tacc=0.9049\t9676.9 examples/second\n","[Step=150]\tLoss=0.2856\tacc=0.9039\t9738.1 examples/second\n","Test Loss=0.3831, Test acc=0.8698\n","Saving...\n","\n","Epoch: 12\n","[Step=50]\tLoss=0.2718\tacc=0.9090\t6495.3 examples/second\n","[Step=100]\tLoss=0.2715\tacc=0.9085\t9711.5 examples/second\n","[Step=150]\tLoss=0.2718\tacc=0.9080\t9779.9 examples/second\n","Test Loss=0.3730, Test acc=0.8736\n","Saving...\n","\n","Epoch: 13\n","[Step=50]\tLoss=0.2718\tacc=0.9081\t6356.9 examples/second\n","[Step=100]\tLoss=0.2623\tacc=0.9121\t10072.4 examples/second\n","[Step=150]\tLoss=0.2582\tacc=0.9136\t9751.3 examples/second\n","Test Loss=0.3673, Test acc=0.8744\n","Saving...\n","\n","Epoch: 14\n","[Step=50]\tLoss=0.2572\tacc=0.9116\t6688.3 examples/second\n","[Step=100]\tLoss=0.2531\tacc=0.9137\t9608.2 examples/second\n","[Step=150]\tLoss=0.2523\tacc=0.9138\t10021.0 examples/second\n","Test Loss=0.3630, Test acc=0.8770\n","Saving...\n","\n","Epoch: 15\n","[Step=50]\tLoss=0.2394\tacc=0.9178\t6454.7 examples/second\n","[Step=100]\tLoss=0.2456\tacc=0.9152\t10539.6 examples/second\n","[Step=150]\tLoss=0.2455\tacc=0.9155\t10170.7 examples/second\n","Test Loss=0.3583, Test acc=0.8782\n","Saving...\n","\n","Epoch: 16\n","[Step=50]\tLoss=0.2375\tacc=0.9217\t6502.0 examples/second\n","[Step=100]\tLoss=0.2397\tacc=0.9191\t9764.5 examples/second\n","[Step=150]\tLoss=0.2375\tacc=0.9204\t9309.7 examples/second\n","Test Loss=0.3550, Test acc=0.8795\n","Saving...\n","\n","Epoch: 17\n","[Step=50]\tLoss=0.2432\tacc=0.9155\t6747.4 examples/second\n","[Step=100]\tLoss=0.2392\tacc=0.9177\t10660.6 examples/second\n","[Step=150]\tLoss=0.2364\tacc=0.9192\t10844.7 examples/second\n","Test Loss=0.3521, Test acc=0.8817\n","Saving...\n","\n","Epoch: 18\n","[Step=50]\tLoss=0.2219\tacc=0.9237\t6377.4 examples/second\n","[Step=100]\tLoss=0.2263\tacc=0.9228\t9551.1 examples/second\n","[Step=150]\tLoss=0.2285\tacc=0.9224\t9901.8 examples/second\n","Test Loss=0.3495, Test acc=0.8831\n","Saving...\n","\n","Epoch: 19\n","[Step=50]\tLoss=0.2289\tacc=0.9232\t6341.3 examples/second\n","[Step=100]\tLoss=0.2283\tacc=0.9226\t9758.1 examples/second\n","[Step=150]\tLoss=0.2233\tacc=0.9239\t10727.2 examples/second\n","Test Loss=0.3483, Test acc=0.8841\n","Saving...\n"]}],"source":["net.load_state_dict(torch.load(os.path.join(CODE_PATH, \"pretrained_model.pt\")))\n","best_acc = 0.\n","\n","for epoch in range(20):\n","    print(f\"\\nEpoch: {epoch}\")\n","    net.train()\n","\n","    if epoch < 10:\n","        q = 8 * (epoch + 1)\n","        print(f\"Global Pruning {q:.1f}% of weights before Epoch {epoch}\")\n","        global_prune_by_percentage(net, q=q)\n","\n","    if epoch<9:\n","        finetune_after_prune(net, trainloader, criterion, optimizer,prune=False)\n","    else:\n","        finetune_after_prune(net, trainloader, criterion, optimizer)\n","\n","    #Start the testing code.\n","    net.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(testloader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = net(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            test_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","    num_val_steps = len(testloader)\n","    val_acc = correct / total\n","    print(\"Test Loss=%.4f, Test acc=%.4f\" % (test_loss / (num_val_steps), val_acc))\n","\n","    if epoch>=10:\n","        if val_acc > best_acc:\n","            best_acc = val_acc\n","            print(\"Saving...\")\n","            torch.save(net.state_dict(), os.path.join(CODE_PATH, \"net_after_global_iterative_prune.pt\"))"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oHvVIc-yEuU1","executionInfo":{"status":"ok","timestamp":1743304930934,"user_tz":240,"elapsed":2103,"user":{"displayName":"Luopeiwen Yi","userId":"10397336716270013892"}},"outputId":"630ada7b-6077-421e-914a-f21bd5fb1046"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sparsity of head_conv.0.conv: 0.3101851851851852\n","Sparsity of body_op.0.conv1.0.conv: 0.6566840277777778\n","Sparsity of body_op.0.conv2.0.conv: 0.6393229166666666\n","Sparsity of body_op.1.conv1.0.conv: 0.6271701388888888\n","Sparsity of body_op.1.conv2.0.conv: 0.6484375\n","Sparsity of body_op.2.conv1.0.conv: 0.6315104166666666\n","Sparsity of body_op.2.conv2.0.conv: 0.6671006944444444\n","Sparsity of body_op.3.conv1.0.conv: 0.6245659722222222\n","Sparsity of body_op.3.conv2.0.conv: 0.6885850694444444\n","Sparsity of body_op.4.conv1.0.conv: 0.7253689236111112\n","Sparsity of body_op.4.conv2.0.conv: 0.7825520833333334\n","Sparsity of body_op.5.conv1.0.conv: 0.7243923611111112\n","Sparsity of body_op.5.conv2.0.conv: 0.8129340277777778\n","Sparsity of body_op.6.conv1.0.conv: 0.732421875\n","Sparsity of body_op.6.conv2.0.conv: 0.7647569444444444\n","Sparsity of body_op.7.conv1.0.conv: 0.7768825954861112\n","Sparsity of body_op.7.conv2.0.conv: 0.8261176215277778\n","Sparsity of body_op.8.conv1.0.conv: 0.852783203125\n","Sparsity of body_op.8.conv2.0.conv: 0.9767252604166666\n","Sparsity of final_fc.linear: 0.15625\n","Total sparsity of: 0.7999970186631685\n","Test Loss=0.3483, Test accuracy=0.8841\n"]}],"source":["net.load_state_dict(torch.load(os.path.join(CODE_PATH, \"net_after_global_iterative_prune.pt\")))\n","\n","zeros_sum = 0\n","total_sum = 0\n","for name,layer in net.named_modules():\n","    if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n","        np_weight = layer.weight.data.cpu().numpy()\n","        zeros = np.sum(np_weight == 0)\n","        total = np_weight.size\n","        zeros_sum += zeros\n","        total_sum += total\n","        print('Sparsity of '+name+': '+str(zeros/total))\n","print('Total sparsity of: '+str(zeros_sum/total_sum))\n","test(net)"]},{"cell_type":"markdown","source":["Test Accuracy of Global Iterative Pruning + Fine-tuning at 80% sparsity level: 88.41%"],"metadata":{"id":"kOD5gH2QEa15"}},{"cell_type":"markdown","source":["### Comparison of Pruning Strategies\n","\n","| Feature                  | One-Shot Layer-wise Pruning      | Iterative Layer-wise Pruning      | Global Iterative Pruning                     |\n","|--------------------------|----------------------------------|-----------------------------------|-----------------------------------------------|\n","| When pruning happens     | Once before fine-tuning          | Gradually before each epoch       | Gradually before each epoch                   |\n","| Threshold basis          | Per-layer q-th percentile        | Per-layer q-th percentile         | Global q-th percentile across all layers      |\n","| Sparsity per layer       | Equal (e.g., 70% each)           | Equal (e.g., 8×e% each epoch)     | Unequal (depends on global threshold)         |\n","| Flexibility              | Low                              | Moderate                          | High                                          |\n","| Implementation           | Simple                           | Moderate                          | More complex but globally aware               |\n","| Use-case benefit         | Fast benchmark                   | Controlled gradual sparsity       | Smarter pruning, better overall performance   |\n"],"metadata":{"id":"jIVZ2e4snt32"}},{"cell_type":"markdown","source":["## Quantization"],"metadata":{"id":"6TBAP8t16Amy"}},{"cell_type":"markdown","source":["Besides pruning, fixed-point quantization is another important technique applied for deep neural network compression. In this Lab, you will convert the ResNet-20 model we used in previous lab into a quantized model, evaluate is performance and apply finetuning on the model."],"metadata":{"id":"aDNKNtor6QVv"}},{"cell_type":"markdown","source":["#### Implement STE function (FP_layers.py)"],"metadata":{"id":"hjDnteRM9sLY"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","import numpy as np\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","class STE(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, w, bit, symmetric=False):\n","        '''\n","        symmetric: True for symmetric quantization, False for asymmetric quantization\n","        '''\n","        if bit is None:\n","            wq = w\n","        elif bit == 0:\n","            wq = w * 0\n","        else:\n","            # Build a mask to record position of zero weights\n","            weight_mask = (w != 0).float()\n","\n","            if symmetric == False:\n","                # Compute alpha (scale) for dynamic scaling\n","                w_min = w.min()\n","                w_max = w.max()\n","                alpha = w_max - w_min\n","                beta = w_min\n","\n","                # Scale w with alpha and beta so that all elements in ws are between 0 and 1\n","                ws = (w - beta) / (alpha + 1e-8)\n","\n","                step = 2 ** bit - 1\n","                # Quantize ws with a linear quantizer to \"bit\" bits\n","                R = torch.round(ws * step) / step\n","\n","                # Scale the quantized weight R back with alpha and beta\n","                wq = R * alpha + beta\n","\n","            else:\n","                # Symmetric quantization (not implemented here)\n","                wq = w  # Placeholder, for Lab 4\n","\n","            # Restore zero elements in wq\n","            wq = wq * weight_mask\n","\n","        return wq\n","\n","    @staticmethod\n","    def backward(ctx, g):\n","        return g, None, None\n","\n","class FP_Linear(nn.Module):\n","    def __init__(self, in_features, out_features, Nbits=None, symmetric=False):\n","        super(FP_Linear, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.linear = nn.Linear(in_features, out_features)\n","        self.Nbits = Nbits\n","        self.symmetric = symmetric\n","\n","        # Initailization\n","        m = self.in_features\n","        n = self.out_features\n","        self.linear.weight.data.normal_(0, math.sqrt(2. / (m + n)))\n","\n","    def forward(self, x):\n","        return F.linear(x, STE.apply(self.linear.weight, self.Nbits, self.symmetric), self.linear.bias)\n","\n","class FP_Conv(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=False, Nbits=None, symmetric=False):\n","        super(FP_Conv, self).__init__()\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.kernel_size = kernel_size\n","        self.stride = stride\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias)\n","        self.Nbits = Nbits\n","        self.symmetric = symmetric\n","\n","        # Initialization\n","        n = self.kernel_size * self.kernel_size * self.out_channels\n","        m = self.kernel_size * self.kernel_size * self.in_channels\n","        self.conv.weight.data.normal_(0, math.sqrt(2. / (n + m)))\n","        self.sparsity = 1.0\n","\n","    def forward(self, x):\n","        return F.conv2d(x, STE.apply(self.conv.weight, self.Nbits, self.symmetric), self.conv.bias, self.conv.stride, self.conv.padding, self.conv.dilation, self.conv.groups)"],"metadata":{"id":"a0Nj5VaK6LyI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JlX7p5EyEuU1"},"source":["#### Asymmetric Fixed-point quantization"]},{"cell_type":"code","source":["bit_list = [6, 5, 4, 3, 2]\n","for Nbits in bit_list:\n","    print(f\"\\nTesting ResNet-20 with Nbits = {Nbits} (residual blocks only)\")\n","    # Only residual blocks are quantized; first conv and final FC are still FP\n","    net = ResNetCIFAR(num_layers=20, Nbits=Nbits)\n","    net.load_state_dict(torch.load(os.path.join(CODE_PATH, \"pretrained_model.pt\")))\n","    net = net.to(device)\n","    test(net)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fm-wwrbuCiFS","executionInfo":{"status":"ok","timestamp":1743338688188,"user_tz":240,"elapsed":10687,"user":{"displayName":"Luopeiwen Yi","userId":"10397336716270013892"}},"outputId":"bf31ad9b-d70a-48a5-8a44-98b1d02119b0"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Testing ResNet-20 with Nbits = 6 (residual blocks only)\n","Test Loss=0.3365, Test accuracy=0.9144\n","\n","Testing ResNet-20 with Nbits = 5 (residual blocks only)\n","Test Loss=0.3391, Test accuracy=0.9113\n","\n","Testing ResNet-20 with Nbits = 4 (residual blocks only)\n","Test Loss=0.3862, Test accuracy=0.8973\n","\n","Testing ResNet-20 with Nbits = 3 (residual blocks only)\n","Test Loss=0.9869, Test accuracy=0.7660\n","\n","Testing ResNet-20 with Nbits = 2 (residual blocks only)\n","Test Loss=9.6141, Test accuracy=0.0899\n"]}]},{"cell_type":"code","source":["finetune_bits = [5, 4, 3, 2]\n","\n","for Nbits in finetune_bits:\n","    print(f\"\\n=== Finetuning Quantized Model (Nbits = {Nbits}) ===\")\n","\n","    # Create quantized model\n","    net = ResNetCIFAR(num_layers=20, Nbits=Nbits)\n","    net.load_state_dict(torch.load(os.path.join(CODE_PATH, \"pretrained_model.pt\")))\n","    net = net.to(device)\n","\n","    # Finetune for 20 epochs\n","    finetune(net, epochs=20, batch_size=256, lr=0.002, reg=1e-4)\n","\n","    # Load best model after finetune\n","    net.load_state_dict(torch.load(\"quantized_net_after_finetune.pt\"))\n","\n","    # save this to local path\n","    torch.save(net.state_dict(), os.path.join(CODE_PATH, f\"quantized_net_after_finetune_Nbits_{Nbits}.pt\"))\n","\n","    print(f\"Test accuracy after finetuning (Nbits = {Nbits}):\")\n","    test(net)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nYIFLDJLCxxt","executionInfo":{"status":"ok","timestamp":1743339352788,"user_tz":240,"elapsed":622769,"user":{"displayName":"Luopeiwen Yi","userId":"10397336716270013892"}},"outputId":"4406d83d-7c13-4c6e-ec93-dbb671d70ff0"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Finetuning Quantized Model (Nbits = 5) ===\n","==> Preparing data..\n","\n","Epoch: 0\n","[Step=50]\tLoss=0.0488\tacc=0.9841\t6203.2 examples/second\n","[Step=100]\tLoss=0.0511\tacc=0.9830\t8847.3 examples/second\n","[Step=150]\tLoss=0.0516\tacc=0.9829\t9115.7 examples/second\n","Test Loss=0.3284, Test acc=0.9132\n","Saving...\n","\n","Epoch: 1\n","[Step=200]\tLoss=0.0453\tacc=0.9863\t3766.7 examples/second\n","[Step=250]\tLoss=0.0503\tacc=0.9837\t8454.6 examples/second\n","[Step=300]\tLoss=0.0521\tacc=0.9824\t9200.8 examples/second\n","[Step=350]\tLoss=0.0517\tacc=0.9825\t8874.7 examples/second\n","Test Loss=0.3265, Test acc=0.9134\n","Saving...\n","\n","Epoch: 2\n","[Step=400]\tLoss=0.0562\tacc=0.9834\t3796.1 examples/second\n","[Step=450]\tLoss=0.0488\tacc=0.9841\t9310.7 examples/second\n","[Step=500]\tLoss=0.0506\tacc=0.9831\t8655.9 examples/second\n","[Step=550]\tLoss=0.0516\tacc=0.9832\t8640.5 examples/second\n","Test Loss=0.3263, Test acc=0.9142\n","Saving...\n","\n","Epoch: 3\n","[Step=600]\tLoss=0.0533\tacc=0.9837\t3719.4 examples/second\n","[Step=650]\tLoss=0.0501\tacc=0.9839\t9488.1 examples/second\n","[Step=700]\tLoss=0.0496\tacc=0.9840\t9465.8 examples/second\n","[Step=750]\tLoss=0.0513\tacc=0.9831\t9322.2 examples/second\n","Test Loss=0.3260, Test acc=0.9148\n","Saving...\n","\n","Epoch: 4\n","[Step=800]\tLoss=0.0464\tacc=0.9839\t3844.9 examples/second\n","[Step=850]\tLoss=0.0485\tacc=0.9862\t8633.7 examples/second\n","[Step=900]\tLoss=0.0483\tacc=0.9857\t8246.6 examples/second\n","[Step=950]\tLoss=0.0494\tacc=0.9851\t8733.9 examples/second\n","Test Loss=0.3288, Test acc=0.9139\n","\n","Epoch: 5\n","[Step=1000]\tLoss=0.0523\tacc=0.9840\t3751.9 examples/second\n","[Step=1050]\tLoss=0.0529\tacc=0.9831\t8996.5 examples/second\n","[Step=1100]\tLoss=0.0522\tacc=0.9836\t9253.8 examples/second\n","[Step=1150]\tLoss=0.0503\tacc=0.9843\t9434.5 examples/second\n","Test Loss=0.3284, Test acc=0.9145\n","\n","Epoch: 6\n","[Step=1200]\tLoss=0.0511\tacc=0.9837\t3735.0 examples/second\n","[Step=1250]\tLoss=0.0496\tacc=0.9842\t8623.1 examples/second\n","[Step=1300]\tLoss=0.0505\tacc=0.9839\t8906.5 examples/second\n","[Step=1350]\tLoss=0.0505\tacc=0.9839\t9044.5 examples/second\n","Test Loss=0.3292, Test acc=0.9156\n","Saving...\n","\n","Epoch: 7\n","[Step=1400]\tLoss=0.0467\tacc=0.9852\t3772.6 examples/second\n","[Step=1450]\tLoss=0.0482\tacc=0.9853\t9360.7 examples/second\n","[Step=1500]\tLoss=0.0486\tacc=0.9846\t9265.1 examples/second\n","[Step=1550]\tLoss=0.0487\tacc=0.9844\t8623.1 examples/second\n","Test Loss=0.3274, Test acc=0.9151\n","\n","Epoch: 8\n","[Step=1600]\tLoss=0.0483\tacc=0.9855\t3728.3 examples/second\n","[Step=1650]\tLoss=0.0463\tacc=0.9861\t9152.0 examples/second\n","[Step=1700]\tLoss=0.0472\tacc=0.9855\t8978.6 examples/second\n","[Step=1750]\tLoss=0.0470\tacc=0.9852\t9419.6 examples/second\n","Test Loss=0.3287, Test acc=0.9141\n","\n","Epoch: 9\n","[Step=1800]\tLoss=0.0507\tacc=0.9843\t3740.3 examples/second\n","[Step=1850]\tLoss=0.0496\tacc=0.9844\t8275.7 examples/second\n","[Step=1900]\tLoss=0.0500\tacc=0.9842\t8782.8 examples/second\n","[Step=1950]\tLoss=0.0491\tacc=0.9848\t9257.7 examples/second\n","Test Loss=0.3262, Test acc=0.9150\n","\n","Epoch: 10\n","[Step=2000]\tLoss=0.0493\tacc=0.9844\t3772.3 examples/second\n","[Step=2050]\tLoss=0.0498\tacc=0.9839\t8675.3 examples/second\n","[Step=2100]\tLoss=0.0489\tacc=0.9842\t8731.8 examples/second\n","[Step=2150]\tLoss=0.0491\tacc=0.9841\t9377.7 examples/second\n","Test Loss=0.3299, Test acc=0.9136\n","\n","Epoch: 11\n","[Step=2200]\tLoss=0.0465\tacc=0.9864\t3713.9 examples/second\n","[Step=2250]\tLoss=0.0458\tacc=0.9864\t9331.9 examples/second\n","[Step=2300]\tLoss=0.0475\tacc=0.9855\t9351.2 examples/second\n","[Step=2350]\tLoss=0.0474\tacc=0.9853\t9484.3 examples/second\n","Test Loss=0.3309, Test acc=0.9133\n","\n","Epoch: 12\n","[Step=2400]\tLoss=0.0458\tacc=0.9845\t3847.0 examples/second\n","[Step=2450]\tLoss=0.0457\tacc=0.9851\t8438.0 examples/second\n","[Step=2500]\tLoss=0.0469\tacc=0.9845\t8052.2 examples/second\n","Test Loss=0.3273, Test acc=0.9129\n","\n","Epoch: 13\n","[Step=2550]\tLoss=0.0509\tacc=0.9785\t3887.8 examples/second\n","[Step=2600]\tLoss=0.0494\tacc=0.9825\t8800.6 examples/second\n","[Step=2650]\tLoss=0.0468\tacc=0.9848\t9171.8 examples/second\n","[Step=2700]\tLoss=0.0472\tacc=0.9847\t9003.6 examples/second\n","Test Loss=0.3292, Test acc=0.9144\n","\n","Epoch: 14\n","[Step=2750]\tLoss=0.0533\tacc=0.9837\t3747.1 examples/second\n","[Step=2800]\tLoss=0.0469\tacc=0.9846\t8421.7 examples/second\n","[Step=2850]\tLoss=0.0459\tacc=0.9852\t9021.9 examples/second\n","[Step=2900]\tLoss=0.0459\tacc=0.9855\t9217.5 examples/second\n","Test Loss=0.3324, Test acc=0.9124\n","\n","Epoch: 15\n","[Step=2950]\tLoss=0.0517\tacc=0.9789\t3862.0 examples/second\n","[Step=3000]\tLoss=0.0514\tacc=0.9827\t9072.2 examples/second\n","[Step=3050]\tLoss=0.0484\tacc=0.9846\t8937.5 examples/second\n","[Step=3100]\tLoss=0.0470\tacc=0.9852\t9288.4 examples/second\n","Test Loss=0.3301, Test acc=0.9142\n","\n","Epoch: 16\n","[Step=3150]\tLoss=0.0466\tacc=0.9852\t3787.5 examples/second\n","[Step=3200]\tLoss=0.0442\tacc=0.9867\t9140.6 examples/second\n","[Step=3250]\tLoss=0.0452\tacc=0.9863\t9210.8 examples/second\n","[Step=3300]\tLoss=0.0453\tacc=0.9861\t9014.1 examples/second\n","Test Loss=0.3332, Test acc=0.9143\n","\n","Epoch: 17\n","[Step=3350]\tLoss=0.0416\tacc=0.9889\t3864.0 examples/second\n","[Step=3400]\tLoss=0.0482\tacc=0.9854\t8540.0 examples/second\n","[Step=3450]\tLoss=0.0489\tacc=0.9848\t8396.0 examples/second\n","[Step=3500]\tLoss=0.0490\tacc=0.9846\t9189.8 examples/second\n","Test Loss=0.3334, Test acc=0.9132\n","\n","Epoch: 18\n","[Step=3550]\tLoss=0.0483\tacc=0.9837\t3769.7 examples/second\n","[Step=3600]\tLoss=0.0475\tacc=0.9846\t8923.9 examples/second\n","[Step=3650]\tLoss=0.0456\tacc=0.9857\t9641.6 examples/second\n","[Step=3700]\tLoss=0.0462\tacc=0.9854\t9257.0 examples/second\n","Test Loss=0.3340, Test acc=0.9133\n","\n","Epoch: 19\n","[Step=3750]\tLoss=0.0454\tacc=0.9865\t3735.3 examples/second\n","[Step=3800]\tLoss=0.0442\tacc=0.9860\t8851.5 examples/second\n","[Step=3850]\tLoss=0.0457\tacc=0.9850\t8765.9 examples/second\n","[Step=3900]\tLoss=0.0445\tacc=0.9855\t9252.4 examples/second\n","Test Loss=0.3312, Test acc=0.9151\n","Test accuracy after finetuning (Nbits = 5):\n","Test Loss=0.3292, Test accuracy=0.9156\n","\n","=== Finetuning Quantized Model (Nbits = 4) ===\n","==> Preparing data..\n","\n","Epoch: 0\n","[Step=50]\tLoss=0.0682\tacc=0.9765\t6089.5 examples/second\n","[Step=100]\tLoss=0.0688\tacc=0.9755\t8748.9 examples/second\n","[Step=150]\tLoss=0.0665\tacc=0.9767\t8487.5 examples/second\n","Test Loss=0.3378, Test acc=0.9086\n","Saving...\n","\n","Epoch: 1\n","[Step=200]\tLoss=0.0770\tacc=0.9775\t3841.3 examples/second\n","[Step=250]\tLoss=0.0654\tacc=0.9787\t8522.8 examples/second\n","[Step=300]\tLoss=0.0629\tacc=0.9799\t9191.0 examples/second\n","[Step=350]\tLoss=0.0622\tacc=0.9798\t8921.0 examples/second\n","Test Loss=0.3363, Test acc=0.9104\n","Saving...\n","\n","Epoch: 2\n","[Step=400]\tLoss=0.0600\tacc=0.9839\t3658.7 examples/second\n","[Step=450]\tLoss=0.0599\tacc=0.9799\t9095.4 examples/second\n","[Step=500]\tLoss=0.0598\tacc=0.9801\t9188.8 examples/second\n","[Step=550]\tLoss=0.0595\tacc=0.9800\t8967.2 examples/second\n","Test Loss=0.3359, Test acc=0.9122\n","Saving...\n","\n","Epoch: 3\n","[Step=600]\tLoss=0.0592\tacc=0.9801\t3794.8 examples/second\n","[Step=650]\tLoss=0.0599\tacc=0.9795\t8805.5 examples/second\n","[Step=700]\tLoss=0.0586\tacc=0.9802\t8617.2 examples/second\n","[Step=750]\tLoss=0.0590\tacc=0.9800\t8324.9 examples/second\n","Test Loss=0.3335, Test acc=0.9131\n","Saving...\n","\n","Epoch: 4\n","[Step=800]\tLoss=0.0622\tacc=0.9771\t3820.7 examples/second\n","[Step=850]\tLoss=0.0574\tacc=0.9805\t8722.7 examples/second\n","[Step=900]\tLoss=0.0591\tacc=0.9798\t9123.7 examples/second\n","[Step=950]\tLoss=0.0588\tacc=0.9801\t9363.0 examples/second\n","Test Loss=0.3325, Test acc=0.9120\n","\n","Epoch: 5\n","[Step=1000]\tLoss=0.0612\tacc=0.9795\t3647.6 examples/second\n","[Step=1050]\tLoss=0.0595\tacc=0.9809\t8886.3 examples/second\n","[Step=1100]\tLoss=0.0603\tacc=0.9801\t9365.8 examples/second\n","[Step=1150]\tLoss=0.0590\tacc=0.9803\t9094.7 examples/second\n","Test Loss=0.3316, Test acc=0.9125\n","\n","Epoch: 6\n","[Step=1200]\tLoss=0.0541\tacc=0.9823\t3755.2 examples/second\n","[Step=1250]\tLoss=0.0571\tacc=0.9812\t8985.1 examples/second\n","[Step=1300]\tLoss=0.0572\tacc=0.9809\t8612.7 examples/second\n","[Step=1350]\tLoss=0.0576\tacc=0.9808\t8780.7 examples/second\n","Test Loss=0.3329, Test acc=0.9125\n","\n","Epoch: 7\n","[Step=1400]\tLoss=0.0592\tacc=0.9794\t3732.7 examples/second\n","[Step=1450]\tLoss=0.0566\tacc=0.9808\t9168.5 examples/second\n","[Step=1500]\tLoss=0.0572\tacc=0.9805\t8850.8 examples/second\n","[Step=1550]\tLoss=0.0573\tacc=0.9807\t8992.4 examples/second\n","Test Loss=0.3309, Test acc=0.9138\n","Saving...\n","\n","Epoch: 8\n","[Step=1600]\tLoss=0.0552\tacc=0.9830\t3773.4 examples/second\n","[Step=1650]\tLoss=0.0592\tacc=0.9810\t8492.6 examples/second\n","[Step=1700]\tLoss=0.0577\tacc=0.9809\t8797.7 examples/second\n","[Step=1750]\tLoss=0.0588\tacc=0.9805\t9166.8 examples/second\n","Test Loss=0.3292, Test acc=0.9127\n","\n","Epoch: 9\n","[Step=1800]\tLoss=0.0586\tacc=0.9793\t3713.7 examples/second\n","[Step=1850]\tLoss=0.0556\tacc=0.9812\t8938.7 examples/second\n","[Step=1900]\tLoss=0.0548\tacc=0.9819\t9185.9 examples/second\n","[Step=1950]\tLoss=0.0547\tacc=0.9818\t9180.8 examples/second\n","Test Loss=0.3346, Test acc=0.9126\n","\n","Epoch: 10\n","[Step=2000]\tLoss=0.0573\tacc=0.9821\t3594.9 examples/second\n","[Step=2050]\tLoss=0.0564\tacc=0.9817\t8450.2 examples/second\n","[Step=2100]\tLoss=0.0538\tacc=0.9826\t8606.3 examples/second\n","[Step=2150]\tLoss=0.0534\tacc=0.9827\t8904.2 examples/second\n","Test Loss=0.3310, Test acc=0.9125\n","\n","Epoch: 11\n","[Step=2200]\tLoss=0.0542\tacc=0.9820\t3824.4 examples/second\n","[Step=2250]\tLoss=0.0565\tacc=0.9812\t8639.4 examples/second\n","[Step=2300]\tLoss=0.0565\tacc=0.9816\t8392.2 examples/second\n","[Step=2350]\tLoss=0.0565\tacc=0.9812\t9298.8 examples/second\n","Test Loss=0.3312, Test acc=0.9118\n","\n","Epoch: 12\n","[Step=2400]\tLoss=0.0561\tacc=0.9824\t3857.6 examples/second\n","[Step=2450]\tLoss=0.0532\tacc=0.9831\t8838.9 examples/second\n","[Step=2500]\tLoss=0.0540\tacc=0.9825\t9182.4 examples/second\n","Test Loss=0.3343, Test acc=0.9126\n","\n","Epoch: 13\n","[Step=2550]\tLoss=0.0361\tacc=0.9961\t3865.9 examples/second\n","[Step=2600]\tLoss=0.0530\tacc=0.9832\t8302.1 examples/second\n","[Step=2650]\tLoss=0.0528\tacc=0.9826\t9425.6 examples/second\n","[Step=2700]\tLoss=0.0539\tacc=0.9822\t9599.7 examples/second\n","Test Loss=0.3321, Test acc=0.9140\n","Saving...\n","\n","Epoch: 14\n","[Step=2750]\tLoss=0.0484\tacc=0.9850\t3788.3 examples/second\n","[Step=2800]\tLoss=0.0541\tacc=0.9821\t9044.7 examples/second\n","[Step=2850]\tLoss=0.0540\tacc=0.9819\t8739.4 examples/second\n","[Step=2900]\tLoss=0.0550\tacc=0.9816\t8481.2 examples/second\n","Test Loss=0.3380, Test acc=0.9126\n","\n","Epoch: 15\n","[Step=2950]\tLoss=0.0554\tacc=0.9805\t3699.0 examples/second\n","[Step=3000]\tLoss=0.0543\tacc=0.9820\t8854.9 examples/second\n","[Step=3050]\tLoss=0.0555\tacc=0.9816\t8813.7 examples/second\n","[Step=3100]\tLoss=0.0556\tacc=0.9815\t9067.9 examples/second\n","Test Loss=0.3328, Test acc=0.9131\n","\n","Epoch: 16\n","[Step=3150]\tLoss=0.0539\tacc=0.9824\t3820.9 examples/second\n","[Step=3200]\tLoss=0.0536\tacc=0.9816\t8336.8 examples/second\n","[Step=3250]\tLoss=0.0531\tacc=0.9821\t8343.0 examples/second\n","[Step=3300]\tLoss=0.0529\tacc=0.9826\t8918.3 examples/second\n","Test Loss=0.3361, Test acc=0.9120\n","\n","Epoch: 17\n","[Step=3350]\tLoss=0.0580\tacc=0.9822\t3761.7 examples/second\n","[Step=3400]\tLoss=0.0540\tacc=0.9827\t9173.8 examples/second\n","[Step=3450]\tLoss=0.0560\tacc=0.9813\t9093.2 examples/second\n","[Step=3500]\tLoss=0.0547\tacc=0.9820\t9058.0 examples/second\n","Test Loss=0.3324, Test acc=0.9138\n","\n","Epoch: 18\n","[Step=3550]\tLoss=0.0597\tacc=0.9798\t3630.2 examples/second\n","[Step=3600]\tLoss=0.0532\tacc=0.9823\t9194.1 examples/second\n","[Step=3650]\tLoss=0.0533\tacc=0.9824\t8626.1 examples/second\n","[Step=3700]\tLoss=0.0533\tacc=0.9822\t8619.0 examples/second\n","Test Loss=0.3332, Test acc=0.9126\n","\n","Epoch: 19\n","[Step=3750]\tLoss=0.0536\tacc=0.9821\t3796.2 examples/second\n","[Step=3800]\tLoss=0.0542\tacc=0.9827\t8995.9 examples/second\n","[Step=3850]\tLoss=0.0530\tacc=0.9826\t8580.1 examples/second\n","[Step=3900]\tLoss=0.0524\tacc=0.9830\t8557.1 examples/second\n","Test Loss=0.3325, Test acc=0.9131\n","Test accuracy after finetuning (Nbits = 4):\n","Test Loss=0.3321, Test accuracy=0.9140\n","\n","=== Finetuning Quantized Model (Nbits = 3) ===\n","==> Preparing data..\n","\n","Epoch: 0\n","[Step=50]\tLoss=0.1727\tacc=0.9404\t6315.6 examples/second\n","[Step=100]\tLoss=0.1620\tacc=0.9432\t9465.2 examples/second\n","[Step=150]\tLoss=0.1543\tacc=0.9455\t8666.2 examples/second\n","Test Loss=0.3896, Test acc=0.8952\n","Saving...\n","\n","Epoch: 1\n","[Step=200]\tLoss=0.1489\tacc=0.9414\t3794.6 examples/second\n","[Step=250]\tLoss=0.1349\tacc=0.9496\t8968.1 examples/second\n","[Step=300]\tLoss=0.1251\tacc=0.9546\t9282.9 examples/second\n","[Step=350]\tLoss=0.1247\tacc=0.9548\t9288.3 examples/second\n","Test Loss=0.3872, Test acc=0.8969\n","Saving...\n","\n","Epoch: 2\n","[Step=400]\tLoss=0.1171\tacc=0.9575\t3880.2 examples/second\n","[Step=450]\tLoss=0.1122\tacc=0.9584\t8477.7 examples/second\n","[Step=500]\tLoss=0.1144\tacc=0.9583\t8629.1 examples/second\n","[Step=550]\tLoss=0.1126\tacc=0.9588\t9323.3 examples/second\n","Test Loss=0.3795, Test acc=0.9002\n","Saving...\n","\n","Epoch: 3\n","[Step=600]\tLoss=0.1018\tacc=0.9616\t3921.6 examples/second\n","[Step=650]\tLoss=0.1063\tacc=0.9623\t9229.0 examples/second\n","[Step=700]\tLoss=0.1057\tacc=0.9621\t8824.9 examples/second\n","[Step=750]\tLoss=0.1070\tacc=0.9613\t8892.9 examples/second\n","Test Loss=0.3701, Test acc=0.9024\n","Saving...\n","\n","Epoch: 4\n","[Step=800]\tLoss=0.1132\tacc=0.9570\t3631.9 examples/second\n","[Step=850]\tLoss=0.1079\tacc=0.9606\t9034.2 examples/second\n","[Step=900]\tLoss=0.1069\tacc=0.9613\t8953.7 examples/second\n","[Step=950]\tLoss=0.1076\tacc=0.9610\t8968.2 examples/second\n","Test Loss=0.3677, Test acc=0.9027\n","Saving...\n","\n","Epoch: 5\n","[Step=1000]\tLoss=0.1002\tacc=0.9621\t3789.6 examples/second\n","[Step=1050]\tLoss=0.1030\tacc=0.9629\t8862.9 examples/second\n","[Step=1100]\tLoss=0.1042\tacc=0.9626\t8770.8 examples/second\n","[Step=1150]\tLoss=0.1005\tacc=0.9636\t8514.3 examples/second\n","Test Loss=0.3661, Test acc=0.9011\n","\n","Epoch: 6\n","[Step=1200]\tLoss=0.1021\tacc=0.9653\t3868.6 examples/second\n","[Step=1250]\tLoss=0.1015\tacc=0.9635\t9036.9 examples/second\n","[Step=1300]\tLoss=0.1025\tacc=0.9635\t9102.7 examples/second\n","[Step=1350]\tLoss=0.1018\tacc=0.9635\t9260.8 examples/second\n","Test Loss=0.3713, Test acc=0.9019\n","\n","Epoch: 7\n","[Step=1400]\tLoss=0.1085\tacc=0.9601\t3823.5 examples/second\n","[Step=1450]\tLoss=0.0996\tacc=0.9640\t8431.0 examples/second\n","[Step=1500]\tLoss=0.1009\tacc=0.9633\t8787.1 examples/second\n","[Step=1550]\tLoss=0.0994\tacc=0.9639\t9000.7 examples/second\n","Test Loss=0.3598, Test acc=0.9031\n","Saving...\n","\n","Epoch: 8\n","[Step=1600]\tLoss=0.0950\tacc=0.9645\t3750.2 examples/second\n","[Step=1650]\tLoss=0.0955\tacc=0.9643\t9236.0 examples/second\n","[Step=1700]\tLoss=0.0965\tacc=0.9640\t9200.2 examples/second\n","[Step=1750]\tLoss=0.0963\tacc=0.9641\t9050.2 examples/second\n","Test Loss=0.3599, Test acc=0.9037\n","Saving...\n","\n","Epoch: 9\n","[Step=1800]\tLoss=0.1007\tacc=0.9639\t3677.9 examples/second\n","[Step=1850]\tLoss=0.0959\tacc=0.9654\t8861.5 examples/second\n","[Step=1900]\tLoss=0.0943\tacc=0.9661\t9196.3 examples/second\n","[Step=1950]\tLoss=0.0943\tacc=0.9661\t9314.2 examples/second\n","Test Loss=0.3634, Test acc=0.9042\n","Saving...\n","\n","Epoch: 10\n","[Step=2000]\tLoss=0.0979\tacc=0.9645\t3804.1 examples/second\n","[Step=2050]\tLoss=0.0961\tacc=0.9663\t8709.1 examples/second\n","[Step=2100]\tLoss=0.0954\tacc=0.9661\t8252.8 examples/second\n","[Step=2150]\tLoss=0.0947\tacc=0.9662\t9096.2 examples/second\n","Test Loss=0.3511, Test acc=0.9040\n","\n","Epoch: 11\n","[Step=2200]\tLoss=0.0849\tacc=0.9696\t3838.4 examples/second\n","[Step=2250]\tLoss=0.0907\tacc=0.9672\t8718.0 examples/second\n","[Step=2300]\tLoss=0.0917\tacc=0.9667\t8527.5 examples/second\n","[Step=2350]\tLoss=0.0916\tacc=0.9667\t9085.4 examples/second\n","Test Loss=0.3641, Test acc=0.9042\n","\n","Epoch: 12\n","[Step=2400]\tLoss=0.0883\tacc=0.9692\t3593.9 examples/second\n","[Step=2450]\tLoss=0.0937\tacc=0.9665\t9108.8 examples/second\n","[Step=2500]\tLoss=0.0930\tacc=0.9666\t9280.1 examples/second\n","Test Loss=0.3595, Test acc=0.9049\n","Saving...\n","\n","Epoch: 13\n","[Step=2550]\tLoss=0.0869\tacc=0.9746\t3950.8 examples/second\n","[Step=2600]\tLoss=0.0884\tacc=0.9675\t8657.1 examples/second\n","[Step=2650]\tLoss=0.0886\tacc=0.9676\t8577.7 examples/second\n","[Step=2700]\tLoss=0.0891\tacc=0.9678\t8693.1 examples/second\n","Test Loss=0.3529, Test acc=0.9044\n","\n","Epoch: 14\n","[Step=2750]\tLoss=0.0993\tacc=0.9590\t3840.6 examples/second\n","[Step=2800]\tLoss=0.0919\tacc=0.9655\t9066.6 examples/second\n","[Step=2850]\tLoss=0.0899\tacc=0.9665\t9148.3 examples/second\n","[Step=2900]\tLoss=0.0893\tacc=0.9671\t8997.1 examples/second\n","Test Loss=0.3603, Test acc=0.9058\n","Saving...\n","\n","Epoch: 15\n","[Step=2950]\tLoss=0.0915\tacc=0.9668\t3837.9 examples/second\n","[Step=3000]\tLoss=0.0901\tacc=0.9682\t8641.9 examples/second\n","[Step=3050]\tLoss=0.0870\tacc=0.9696\t8404.6 examples/second\n","[Step=3100]\tLoss=0.0877\tacc=0.9694\t8941.5 examples/second\n","Test Loss=0.3545, Test acc=0.9049\n","\n","Epoch: 16\n","[Step=3150]\tLoss=0.0868\tacc=0.9701\t3861.8 examples/second\n","[Step=3200]\tLoss=0.0862\tacc=0.9699\t8990.3 examples/second\n","[Step=3250]\tLoss=0.0868\tacc=0.9693\t8416.0 examples/second\n","[Step=3300]\tLoss=0.0873\tacc=0.9693\t8682.1 examples/second\n","Test Loss=0.3611, Test acc=0.9047\n","\n","Epoch: 17\n","[Step=3350]\tLoss=0.0815\tacc=0.9714\t3719.4 examples/second\n","[Step=3400]\tLoss=0.0841\tacc=0.9708\t8789.9 examples/second\n","[Step=3450]\tLoss=0.0862\tacc=0.9690\t9312.5 examples/second\n","[Step=3500]\tLoss=0.0873\tacc=0.9686\t9309.0 examples/second\n","Test Loss=0.3691, Test acc=0.9040\n","\n","Epoch: 18\n","[Step=3550]\tLoss=0.0836\tacc=0.9689\t3808.8 examples/second\n","[Step=3600]\tLoss=0.0841\tacc=0.9702\t9140.4 examples/second\n","[Step=3650]\tLoss=0.0877\tacc=0.9688\t8588.9 examples/second\n","[Step=3700]\tLoss=0.0881\tacc=0.9684\t8536.8 examples/second\n","Test Loss=0.3620, Test acc=0.9043\n","\n","Epoch: 19\n","[Step=3750]\tLoss=0.0942\tacc=0.9650\t3858.0 examples/second\n","[Step=3800]\tLoss=0.0876\tacc=0.9686\t8994.9 examples/second\n","[Step=3850]\tLoss=0.0864\tacc=0.9688\t9116.4 examples/second\n","[Step=3900]\tLoss=0.0865\tacc=0.9688\t8926.6 examples/second\n","Test Loss=0.3648, Test acc=0.9030\n","Test accuracy after finetuning (Nbits = 3):\n","Test Loss=0.3603, Test accuracy=0.9058\n","\n","=== Finetuning Quantized Model (Nbits = 2) ===\n","==> Preparing data..\n","\n","Epoch: 0\n","[Step=50]\tLoss=1.3816\tacc=0.6174\t6234.7 examples/second\n","[Step=100]\tLoss=1.0943\tacc=0.6806\t8809.3 examples/second\n","[Step=150]\tLoss=0.9546\tacc=0.7136\t8655.0 examples/second\n","Test Loss=0.7284, Test acc=0.7765\n","Saving...\n","\n","Epoch: 1\n","[Step=200]\tLoss=0.6208\tacc=0.7959\t3856.2 examples/second\n","[Step=250]\tLoss=0.5741\tacc=0.8057\t8545.0 examples/second\n","[Step=300]\tLoss=0.5643\tacc=0.8109\t8343.7 examples/second\n","[Step=350]\tLoss=0.5439\tacc=0.8170\t8759.6 examples/second\n","Test Loss=0.6637, Test acc=0.7988\n","Saving...\n","\n","Epoch: 2\n","[Step=400]\tLoss=0.4751\tacc=0.8335\t3738.2 examples/second\n","[Step=450]\tLoss=0.4824\tacc=0.8347\t9143.4 examples/second\n","[Step=500]\tLoss=0.4712\tacc=0.8375\t8947.7 examples/second\n","[Step=550]\tLoss=0.4671\tacc=0.8385\t8953.8 examples/second\n","Test Loss=0.5800, Test acc=0.8186\n","Saving...\n","\n","Epoch: 3\n","[Step=600]\tLoss=0.4373\tacc=0.8529\t3755.0 examples/second\n","[Step=650]\tLoss=0.4256\tacc=0.8530\t8874.9 examples/second\n","[Step=700]\tLoss=0.4265\tacc=0.8529\t8913.1 examples/second\n","[Step=750]\tLoss=0.4239\tacc=0.8534\t9114.6 examples/second\n","Test Loss=0.5702, Test acc=0.8230\n","Saving...\n","\n","Epoch: 4\n","[Step=800]\tLoss=0.3850\tacc=0.8696\t3776.3 examples/second\n","[Step=850]\tLoss=0.4027\tacc=0.8594\t9049.8 examples/second\n","[Step=900]\tLoss=0.3946\tacc=0.8623\t8863.6 examples/second\n","[Step=950]\tLoss=0.3949\tacc=0.8623\t8290.4 examples/second\n","Test Loss=0.5933, Test acc=0.8204\n","\n","Epoch: 5\n","[Step=1000]\tLoss=0.3991\tacc=0.8609\t3812.3 examples/second\n","[Step=1050]\tLoss=0.3774\tacc=0.8699\t8894.5 examples/second\n","[Step=1100]\tLoss=0.3783\tacc=0.8691\t8968.0 examples/second\n","[Step=1150]\tLoss=0.3753\tacc=0.8704\t9016.6 examples/second\n","Test Loss=0.5266, Test acc=0.8388\n","Saving...\n","\n","Epoch: 6\n","[Step=1200]\tLoss=0.3531\tacc=0.8766\t3723.1 examples/second\n","[Step=1250]\tLoss=0.3563\tacc=0.8758\t8377.1 examples/second\n","[Step=1300]\tLoss=0.3564\tacc=0.8750\t9064.2 examples/second\n","[Step=1350]\tLoss=0.3579\tacc=0.8753\t9338.9 examples/second\n","Test Loss=0.5738, Test acc=0.8306\n","\n","Epoch: 7\n","[Step=1400]\tLoss=0.3506\tacc=0.8753\t3750.8 examples/second\n","[Step=1450]\tLoss=0.3495\tacc=0.8768\t8742.8 examples/second\n","[Step=1500]\tLoss=0.3512\tacc=0.8759\t9322.9 examples/second\n","[Step=1550]\tLoss=0.3530\tacc=0.8755\t9126.1 examples/second\n","Test Loss=0.5870, Test acc=0.8230\n","\n","Epoch: 8\n","[Step=1600]\tLoss=0.3303\tacc=0.8810\t3737.9 examples/second\n","[Step=1650]\tLoss=0.3378\tacc=0.8818\t9164.5 examples/second\n","[Step=1700]\tLoss=0.3378\tacc=0.8810\t8839.4 examples/second\n","[Step=1750]\tLoss=0.3349\tacc=0.8821\t9057.6 examples/second\n","Test Loss=0.5253, Test acc=0.8373\n","\n","Epoch: 9\n","[Step=1800]\tLoss=0.3294\tacc=0.8856\t3664.3 examples/second\n","[Step=1850]\tLoss=0.3263\tacc=0.8848\t8745.9 examples/second\n","[Step=1900]\tLoss=0.3280\tacc=0.8839\t8379.7 examples/second\n","[Step=1950]\tLoss=0.3267\tacc=0.8844\t9272.8 examples/second\n","Test Loss=0.5576, Test acc=0.8330\n","\n","Epoch: 10\n","[Step=2000]\tLoss=0.3244\tacc=0.8862\t3738.1 examples/second\n","[Step=2050]\tLoss=0.3247\tacc=0.8864\t9172.3 examples/second\n","[Step=2100]\tLoss=0.3228\tacc=0.8871\t9093.5 examples/second\n","[Step=2150]\tLoss=0.3193\tacc=0.8891\t9460.3 examples/second\n","Test Loss=0.5964, Test acc=0.8245\n","\n","Epoch: 11\n","[Step=2200]\tLoss=0.3156\tacc=0.8873\t3621.2 examples/second\n","[Step=2250]\tLoss=0.3183\tacc=0.8863\t8652.8 examples/second\n","[Step=2300]\tLoss=0.3183\tacc=0.8864\t8819.8 examples/second\n","[Step=2350]\tLoss=0.3140\tacc=0.8879\t9604.4 examples/second\n","Test Loss=0.5552, Test acc=0.8353\n","\n","Epoch: 12\n","[Step=2400]\tLoss=0.3029\tacc=0.8911\t3726.1 examples/second\n","[Step=2450]\tLoss=0.2989\tacc=0.8931\t9093.7 examples/second\n","[Step=2500]\tLoss=0.3015\tacc=0.8926\t8992.6 examples/second\n","Test Loss=0.4845, Test acc=0.8539\n","Saving...\n","\n","Epoch: 13\n","[Step=2550]\tLoss=0.3196\tacc=0.8945\t3719.7 examples/second\n","[Step=2600]\tLoss=0.2958\tacc=0.8957\t8766.6 examples/second\n","[Step=2650]\tLoss=0.2975\tacc=0.8945\t8826.9 examples/second\n","[Step=2700]\tLoss=0.2995\tacc=0.8943\t9235.3 examples/second\n","Test Loss=0.4617, Test acc=0.8541\n","Saving...\n","\n","Epoch: 14\n","[Step=2750]\tLoss=0.2783\tacc=0.9089\t3837.9 examples/second\n","[Step=2800]\tLoss=0.2870\tacc=0.8982\t8681.9 examples/second\n","[Step=2850]\tLoss=0.2884\tacc=0.8983\t8379.5 examples/second\n","[Step=2900]\tLoss=0.2925\tacc=0.8967\t9088.2 examples/second\n","Test Loss=0.4963, Test acc=0.8451\n","\n","Epoch: 15\n","[Step=2950]\tLoss=0.3104\tacc=0.8934\t3552.2 examples/second\n","[Step=3000]\tLoss=0.2963\tacc=0.8960\t7775.0 examples/second\n","[Step=3050]\tLoss=0.2913\tacc=0.8980\t8803.0 examples/second\n","[Step=3100]\tLoss=0.2896\tacc=0.8983\t9113.2 examples/second\n","Test Loss=0.4590, Test acc=0.8526\n","\n","Epoch: 16\n","[Step=3150]\tLoss=0.2791\tacc=0.9001\t3724.4 examples/second\n","[Step=3200]\tLoss=0.2856\tacc=0.8974\t9242.0 examples/second\n","[Step=3250]\tLoss=0.2877\tacc=0.8967\t9032.7 examples/second\n","[Step=3300]\tLoss=0.2885\tacc=0.8961\t9369.3 examples/second\n","Test Loss=0.5034, Test acc=0.8463\n","\n","Epoch: 17\n","[Step=3350]\tLoss=0.2960\tacc=0.8980\t3784.7 examples/second\n","[Step=3400]\tLoss=0.2903\tacc=0.8982\t8747.5 examples/second\n","[Step=3450]\tLoss=0.2886\tacc=0.8988\t8678.0 examples/second\n","[Step=3500]\tLoss=0.2853\tacc=0.9003\t8540.8 examples/second\n","Test Loss=0.4515, Test acc=0.8597\n","Saving...\n","\n","Epoch: 18\n","[Step=3550]\tLoss=0.2791\tacc=0.8990\t3751.4 examples/second\n","[Step=3600]\tLoss=0.2767\tacc=0.9030\t8965.5 examples/second\n","[Step=3650]\tLoss=0.2803\tacc=0.9017\t8782.7 examples/second\n","[Step=3700]\tLoss=0.2800\tacc=0.9014\t8474.5 examples/second\n","Test Loss=0.6657, Test acc=0.8072\n","\n","Epoch: 19\n","[Step=3750]\tLoss=0.2818\tacc=0.9055\t3797.6 examples/second\n","[Step=3800]\tLoss=0.2741\tacc=0.9060\t9068.0 examples/second\n","[Step=3850]\tLoss=0.2724\tacc=0.9049\t8709.9 examples/second\n","[Step=3900]\tLoss=0.2731\tacc=0.9047\t9018.8 examples/second\n","Test Loss=0.4514, Test acc=0.8565\n","Test accuracy after finetuning (Nbits = 2):\n","Test Loss=0.4515, Test accuracy=0.8597\n"]}]},{"cell_type":"markdown","source":["As precision becomes lower, accuracy becomes lower as well. Finetuning helps with improving test accuracy."],"metadata":{"id":"HifOKivbJFQc"}},{"cell_type":"markdown","metadata":{"id":"yPspswkDEuU1"},"source":["#### Quantize pruned model"]},{"cell_type":"code","source":["# Finetuning on pruned & quantized model\n","finetune_bits = [5, 4, 3, 2]\n","\n","for Nbits in finetune_bits:\n","    print(f\"\\n=== Pruned + Quantized Model (Nbits = {Nbits}) ===\")\n","\n","    # Load pruned model first\n","    net = ResNetCIFAR(num_layers=20, Nbits=Nbits)\n","    net.load_state_dict(torch.load(os.path.join(CODE_PATH, \"net_after_global_iterative_prune.pt\")))\n","    net = net.to(device)\n","\n","    print(\"Accuracy BEFORE finetuning:\")\n","    test(net)\n","\n","    # Finetune the pruned + quantized model\n","    finetune(net, epochs=20, batch_size=256, lr=0.002, reg=1e-4)\n","\n","    # Load the best finetuned version\n","    net.load_state_dict(torch.load(\"quantized_net_after_finetune.pt\"))\n","\n","    print(\"Accuracy AFTER finetuning:\")\n","    test(net)\n","\n","    # Save the combined model for documentation\n","    torch.save(net.state_dict(), os.path.join(CODE_PATH, f\"pruned_quantized_net_Nbits_{Nbits}.pt\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rcbY8z_VLGaH","executionInfo":{"status":"ok","timestamp":1743340952439,"user_tz":240,"elapsed":636719,"user":{"displayName":"Luopeiwen Yi","userId":"10397336716270013892"}},"outputId":"04dd9b2b-19a1-4464-f197-36178662ce14"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Pruned + Quantized Model (Nbits = 5) ===\n","Accuracy BEFORE finetuning:\n","Test Loss=0.3596, Test accuracy=0.8778\n","==> Preparing data..\n","\n","Epoch: 0\n","[Step=50]\tLoss=0.3572\tacc=0.8783\t6143.1 examples/second\n","[Step=100]\tLoss=0.3099\tacc=0.8933\t9034.0 examples/second\n","[Step=150]\tLoss=0.2847\tacc=0.9007\t8874.3 examples/second\n","Test Loss=0.3866, Test acc=0.8789\n","Saving...\n","\n","Epoch: 1\n","[Step=200]\tLoss=0.2337\tacc=0.9170\t3837.0 examples/second\n","[Step=250]\tLoss=0.2124\tacc=0.9244\t9051.0 examples/second\n","[Step=300]\tLoss=0.2112\tacc=0.9240\t8862.6 examples/second\n","[Step=350]\tLoss=0.2094\tacc=0.9251\t8964.5 examples/second\n","Test Loss=0.3600, Test acc=0.8860\n","Saving...\n","\n","Epoch: 2\n","[Step=400]\tLoss=0.1908\tacc=0.9355\t3822.6 examples/second\n","[Step=450]\tLoss=0.1923\tacc=0.9337\t8611.1 examples/second\n","[Step=500]\tLoss=0.1916\tacc=0.9329\t8659.0 examples/second\n","[Step=550]\tLoss=0.1944\tacc=0.9322\t9482.1 examples/second\n","Test Loss=0.3588, Test acc=0.8845\n","\n","Epoch: 3\n","[Step=600]\tLoss=0.1794\tacc=0.9333\t3873.2 examples/second\n","[Step=650]\tLoss=0.1874\tacc=0.9332\t8789.4 examples/second\n","[Step=700]\tLoss=0.1889\tacc=0.9321\t9037.5 examples/second\n","[Step=750]\tLoss=0.1897\tacc=0.9327\t9427.8 examples/second\n","Test Loss=0.3570, Test acc=0.8882\n","Saving...\n","\n","Epoch: 4\n","[Step=800]\tLoss=0.1812\tacc=0.9355\t3669.3 examples/second\n","[Step=850]\tLoss=0.1825\tacc=0.9338\t8939.4 examples/second\n","[Step=900]\tLoss=0.1865\tacc=0.9324\t8968.5 examples/second\n","[Step=950]\tLoss=0.1829\tacc=0.9342\t8792.8 examples/second\n","Test Loss=0.3705, Test acc=0.8872\n","\n","Epoch: 5\n","[Step=1000]\tLoss=0.1806\tacc=0.9348\t3845.1 examples/second\n","[Step=1050]\tLoss=0.1827\tacc=0.9349\t9026.0 examples/second\n","[Step=1100]\tLoss=0.1773\tacc=0.9367\t8586.7 examples/second\n","[Step=1150]\tLoss=0.1759\tacc=0.9375\t8476.9 examples/second\n","Test Loss=0.3590, Test acc=0.8888\n","Saving...\n","\n","Epoch: 6\n","[Step=1200]\tLoss=0.1633\tacc=0.9432\t3648.7 examples/second\n","[Step=1250]\tLoss=0.1681\tacc=0.9415\t9090.9 examples/second\n","[Step=1300]\tLoss=0.1683\tacc=0.9406\t8723.8 examples/second\n","[Step=1350]\tLoss=0.1672\tacc=0.9404\t9024.0 examples/second\n","Test Loss=0.3506, Test acc=0.8919\n","Saving...\n","\n","Epoch: 7\n","[Step=1400]\tLoss=0.1649\tacc=0.9425\t3694.1 examples/second\n","[Step=1450]\tLoss=0.1652\tacc=0.9423\t8837.3 examples/second\n","[Step=1500]\tLoss=0.1609\tacc=0.9434\t8626.0 examples/second\n","[Step=1550]\tLoss=0.1619\tacc=0.9425\t8750.1 examples/second\n","Test Loss=0.3471, Test acc=0.8924\n","Saving...\n","\n","Epoch: 8\n","[Step=1600]\tLoss=0.1523\tacc=0.9448\t3812.8 examples/second\n","[Step=1650]\tLoss=0.1534\tacc=0.9455\t9151.6 examples/second\n","[Step=1700]\tLoss=0.1552\tacc=0.9445\t8894.9 examples/second\n","[Step=1750]\tLoss=0.1557\tacc=0.9443\t8687.9 examples/second\n","Test Loss=0.3384, Test acc=0.8963\n","Saving...\n","\n","Epoch: 9\n","[Step=1800]\tLoss=0.1461\tacc=0.9488\t3752.6 examples/second\n","[Step=1850]\tLoss=0.1512\tacc=0.9459\t8930.0 examples/second\n","[Step=1900]\tLoss=0.1534\tacc=0.9448\t9015.7 examples/second\n","[Step=1950]\tLoss=0.1530\tacc=0.9454\t8841.5 examples/second\n","Test Loss=0.3334, Test acc=0.8970\n","Saving...\n","\n","Epoch: 10\n","[Step=2000]\tLoss=0.1482\tacc=0.9460\t3634.4 examples/second\n","[Step=2050]\tLoss=0.1478\tacc=0.9471\t8386.7 examples/second\n","[Step=2100]\tLoss=0.1506\tacc=0.9468\t9134.6 examples/second\n","[Step=2150]\tLoss=0.1502\tacc=0.9470\t9395.4 examples/second\n","Test Loss=0.3357, Test acc=0.8986\n","Saving...\n","\n","Epoch: 11\n","[Step=2200]\tLoss=0.1415\tacc=0.9474\t3719.7 examples/second\n","[Step=2250]\tLoss=0.1435\tacc=0.9467\t8411.5 examples/second\n","[Step=2300]\tLoss=0.1447\tacc=0.9469\t8337.7 examples/second\n","[Step=2350]\tLoss=0.1447\tacc=0.9471\t9088.2 examples/second\n","Test Loss=0.3362, Test acc=0.8989\n","Saving...\n","\n","Epoch: 12\n","[Step=2400]\tLoss=0.1427\tacc=0.9488\t3653.9 examples/second\n","[Step=2450]\tLoss=0.1407\tacc=0.9505\t8723.9 examples/second\n","[Step=2500]\tLoss=0.1404\tacc=0.9508\t8669.5 examples/second\n","Test Loss=0.3374, Test acc=0.9007\n","Saving...\n","\n","Epoch: 13\n","[Step=2550]\tLoss=0.1093\tacc=0.9609\t3916.3 examples/second\n","[Step=2600]\tLoss=0.1265\tacc=0.9554\t8581.8 examples/second\n","[Step=2650]\tLoss=0.1314\tacc=0.9533\t8466.1 examples/second\n","[Step=2700]\tLoss=0.1322\tacc=0.9527\t8636.2 examples/second\n","Test Loss=0.3345, Test acc=0.9008\n","Saving...\n","\n","Epoch: 14\n","[Step=2750]\tLoss=0.1282\tacc=0.9492\t3788.2 examples/second\n","[Step=2800]\tLoss=0.1334\tacc=0.9521\t8703.0 examples/second\n","[Step=2850]\tLoss=0.1326\tacc=0.9521\t8718.7 examples/second\n","[Step=2900]\tLoss=0.1311\tacc=0.9526\t8943.2 examples/second\n","Test Loss=0.3276, Test acc=0.9012\n","Saving...\n","\n","Epoch: 15\n","[Step=2950]\tLoss=0.1441\tacc=0.9453\t3718.0 examples/second\n","[Step=3000]\tLoss=0.1258\tacc=0.9550\t8498.6 examples/second\n","[Step=3050]\tLoss=0.1277\tacc=0.9554\t8775.1 examples/second\n","[Step=3100]\tLoss=0.1287\tacc=0.9542\t8866.1 examples/second\n","Test Loss=0.3402, Test acc=0.8988\n","\n","Epoch: 16\n","[Step=3150]\tLoss=0.1277\tacc=0.9534\t3751.0 examples/second\n","[Step=3200]\tLoss=0.1242\tacc=0.9556\t9153.6 examples/second\n","[Step=3250]\tLoss=0.1260\tacc=0.9547\t9091.1 examples/second\n","[Step=3300]\tLoss=0.1262\tacc=0.9552\t8947.9 examples/second\n","Test Loss=0.3279, Test acc=0.9014\n","Saving...\n","\n","Epoch: 17\n","[Step=3350]\tLoss=0.1179\tacc=0.9599\t3680.4 examples/second\n","[Step=3400]\tLoss=0.1193\tacc=0.9586\t8648.1 examples/second\n","[Step=3450]\tLoss=0.1204\tacc=0.9576\t8444.6 examples/second\n","[Step=3500]\tLoss=0.1202\tacc=0.9569\t9154.9 examples/second\n","Test Loss=0.3429, Test acc=0.9006\n","\n","Epoch: 18\n","[Step=3550]\tLoss=0.1326\tacc=0.9544\t3726.8 examples/second\n","[Step=3600]\tLoss=0.1216\tacc=0.9580\t8445.0 examples/second\n","[Step=3650]\tLoss=0.1197\tacc=0.9582\t8587.8 examples/second\n","[Step=3700]\tLoss=0.1209\tacc=0.9573\t9081.9 examples/second\n","Test Loss=0.3340, Test acc=0.9015\n","Saving...\n","\n","Epoch: 19\n","[Step=3750]\tLoss=0.1143\tacc=0.9608\t3692.0 examples/second\n","[Step=3800]\tLoss=0.1207\tacc=0.9582\t8951.1 examples/second\n","[Step=3850]\tLoss=0.1187\tacc=0.9582\t9217.4 examples/second\n","[Step=3900]\tLoss=0.1191\tacc=0.9581\t9298.9 examples/second\n","Test Loss=0.3313, Test acc=0.9032\n","Saving...\n","Accuracy AFTER finetuning:\n","Test Loss=0.3313, Test accuracy=0.9032\n","\n","=== Pruned + Quantized Model (Nbits = 4) ===\n","Accuracy BEFORE finetuning:\n","Test Loss=0.4046, Test accuracy=0.8603\n","==> Preparing data..\n","\n","Epoch: 0\n","[Step=50]\tLoss=0.7382\tacc=0.7933\t6284.7 examples/second\n","[Step=100]\tLoss=0.6880\tacc=0.8109\t9216.5 examples/second\n","[Step=150]\tLoss=0.6040\tacc=0.8266\t8770.1 examples/second\n","Test Loss=0.4748, Test acc=0.8423\n","Saving...\n","\n","Epoch: 1\n","[Step=200]\tLoss=0.4002\tacc=0.8594\t3699.9 examples/second\n","[Step=250]\tLoss=0.3569\tacc=0.8848\t8539.5 examples/second\n","[Step=300]\tLoss=0.3512\tacc=0.8861\t8997.3 examples/second\n","[Step=350]\tLoss=0.3440\tacc=0.8875\t8983.3 examples/second\n","Test Loss=0.4207, Test acc=0.8614\n","Saving...\n","\n","Epoch: 2\n","[Step=400]\tLoss=0.3147\tacc=0.8975\t3822.8 examples/second\n","[Step=450]\tLoss=0.3099\tacc=0.8949\t8902.6 examples/second\n","[Step=500]\tLoss=0.3074\tacc=0.8967\t8505.2 examples/second\n","[Step=550]\tLoss=0.3038\tacc=0.8975\t8560.2 examples/second\n","Test Loss=0.4018, Test acc=0.8684\n","Saving...\n","\n","Epoch: 3\n","[Step=600]\tLoss=0.2784\tacc=0.9053\t3766.1 examples/second\n","[Step=650]\tLoss=0.2851\tacc=0.9012\t8798.2 examples/second\n","[Step=700]\tLoss=0.2809\tacc=0.9057\t9053.6 examples/second\n","[Step=750]\tLoss=0.2789\tacc=0.9063\t8805.3 examples/second\n","Test Loss=0.3915, Test acc=0.8736\n","Saving...\n","\n","Epoch: 4\n","[Step=800]\tLoss=0.2490\tacc=0.9170\t3618.7 examples/second\n","[Step=850]\tLoss=0.2596\tacc=0.9123\t9017.9 examples/second\n","[Step=900]\tLoss=0.2616\tacc=0.9096\t8717.0 examples/second\n","[Step=950]\tLoss=0.2590\tacc=0.9105\t8776.4 examples/second\n","Test Loss=0.3796, Test acc=0.8772\n","Saving...\n","\n","Epoch: 5\n","[Step=1000]\tLoss=0.2478\tacc=0.9154\t3728.9 examples/second\n","[Step=1050]\tLoss=0.2444\tacc=0.9160\t8985.2 examples/second\n","[Step=1100]\tLoss=0.2444\tacc=0.9159\t8546.1 examples/second\n","[Step=1150]\tLoss=0.2413\tacc=0.9167\t8178.1 examples/second\n","Test Loss=0.3657, Test acc=0.8818\n","Saving...\n","\n","Epoch: 6\n","[Step=1200]\tLoss=0.2417\tacc=0.9173\t3699.5 examples/second\n","[Step=1250]\tLoss=0.2376\tacc=0.9168\t9150.7 examples/second\n","[Step=1300]\tLoss=0.2355\tacc=0.9174\t8840.9 examples/second\n","[Step=1350]\tLoss=0.2315\tacc=0.9189\t9003.6 examples/second\n","Test Loss=0.3549, Test acc=0.8868\n","Saving...\n","\n","Epoch: 7\n","[Step=1400]\tLoss=0.2157\tacc=0.9212\t3655.0 examples/second\n","[Step=1450]\tLoss=0.2162\tacc=0.9234\t8354.8 examples/second\n","[Step=1500]\tLoss=0.2180\tacc=0.9232\t8574.5 examples/second\n","[Step=1550]\tLoss=0.2177\tacc=0.9239\t8917.1 examples/second\n","Test Loss=0.3555, Test acc=0.8855\n","\n","Epoch: 8\n","[Step=1600]\tLoss=0.2053\tacc=0.9312\t3707.3 examples/second\n","[Step=1650]\tLoss=0.2017\tacc=0.9293\t8349.5 examples/second\n","[Step=1700]\tLoss=0.2018\tacc=0.9293\t8859.6 examples/second\n","[Step=1750]\tLoss=0.2044\tacc=0.9286\t8935.8 examples/second\n","Test Loss=0.3437, Test acc=0.8882\n","Saving...\n","\n","Epoch: 9\n","[Step=1800]\tLoss=0.1986\tacc=0.9314\t3573.3 examples/second\n","[Step=1850]\tLoss=0.2048\tacc=0.9281\t9123.3 examples/second\n","[Step=1900]\tLoss=0.2020\tacc=0.9296\t9178.8 examples/second\n","[Step=1950]\tLoss=0.1996\tacc=0.9295\t9192.5 examples/second\n","Test Loss=0.3424, Test acc=0.8895\n","Saving...\n","\n","Epoch: 10\n","[Step=2000]\tLoss=0.1962\tacc=0.9319\t3665.2 examples/second\n","[Step=2050]\tLoss=0.1952\tacc=0.9326\t8443.8 examples/second\n","[Step=2100]\tLoss=0.1941\tacc=0.9327\t8370.2 examples/second\n","[Step=2150]\tLoss=0.1935\tacc=0.9328\t9200.1 examples/second\n","Test Loss=0.3340, Test acc=0.8922\n","Saving...\n","\n","Epoch: 11\n","[Step=2200]\tLoss=0.1868\tacc=0.9338\t3713.1 examples/second\n","[Step=2250]\tLoss=0.1855\tacc=0.9341\t8731.5 examples/second\n","[Step=2300]\tLoss=0.1844\tacc=0.9345\t8717.1 examples/second\n","[Step=2350]\tLoss=0.1854\tacc=0.9340\t9099.2 examples/second\n","Test Loss=0.3373, Test acc=0.8928\n","Saving...\n","\n","Epoch: 12\n","[Step=2400]\tLoss=0.1767\tacc=0.9351\t3639.5 examples/second\n","[Step=2450]\tLoss=0.1785\tacc=0.9360\t8835.4 examples/second\n","[Step=2500]\tLoss=0.1802\tacc=0.9347\t8917.2 examples/second\n","Test Loss=0.3461, Test acc=0.8943\n","Saving...\n","\n","Epoch: 13\n","[Step=2550]\tLoss=0.2008\tacc=0.9277\t3856.6 examples/second\n","[Step=2600]\tLoss=0.1775\tacc=0.9364\t8744.7 examples/second\n","[Step=2650]\tLoss=0.1770\tacc=0.9380\t8860.5 examples/second\n","[Step=2700]\tLoss=0.1771\tacc=0.9382\t8616.7 examples/second\n","Test Loss=0.3361, Test acc=0.8944\n","Saving...\n","\n","Epoch: 14\n","[Step=2750]\tLoss=0.1698\tacc=0.9447\t3666.2 examples/second\n","[Step=2800]\tLoss=0.1708\tacc=0.9406\t9185.1 examples/second\n","[Step=2850]\tLoss=0.1672\tacc=0.9418\t8702.1 examples/second\n","[Step=2900]\tLoss=0.1687\tacc=0.9409\t8852.1 examples/second\n","Test Loss=0.3401, Test acc=0.8954\n","Saving...\n","\n","Epoch: 15\n","[Step=2950]\tLoss=0.1640\tacc=0.9449\t3577.9 examples/second\n","[Step=3000]\tLoss=0.1607\tacc=0.9426\t8743.6 examples/second\n","[Step=3050]\tLoss=0.1622\tacc=0.9423\t8950.0 examples/second\n","[Step=3100]\tLoss=0.1649\tacc=0.9411\t9120.8 examples/second\n","Test Loss=0.3355, Test acc=0.8965\n","Saving...\n","\n","Epoch: 16\n","[Step=3150]\tLoss=0.1674\tacc=0.9453\t3803.1 examples/second\n","[Step=3200]\tLoss=0.1638\tacc=0.9427\t8962.3 examples/second\n","[Step=3250]\tLoss=0.1628\tacc=0.9431\t9162.0 examples/second\n","[Step=3300]\tLoss=0.1623\tacc=0.9429\t8916.5 examples/second\n","Test Loss=0.3523, Test acc=0.8959\n","\n","Epoch: 17\n","[Step=3350]\tLoss=0.1661\tacc=0.9414\t3705.0 examples/second\n","[Step=3400]\tLoss=0.1585\tacc=0.9444\t9378.6 examples/second\n","[Step=3450]\tLoss=0.1618\tacc=0.9436\t9389.8 examples/second\n","[Step=3500]\tLoss=0.1616\tacc=0.9433\t9118.4 examples/second\n","Test Loss=0.3282, Test acc=0.8994\n","Saving...\n","\n","Epoch: 18\n","[Step=3550]\tLoss=0.1633\tacc=0.9453\t3679.6 examples/second\n","[Step=3600]\tLoss=0.1636\tacc=0.9449\t8766.3 examples/second\n","[Step=3650]\tLoss=0.1599\tacc=0.9454\t8024.0 examples/second\n","[Step=3700]\tLoss=0.1600\tacc=0.9448\t9113.3 examples/second\n","Test Loss=0.3324, Test acc=0.8984\n","\n","Epoch: 19\n","[Step=3750]\tLoss=0.1696\tacc=0.9411\t3799.7 examples/second\n","[Step=3800]\tLoss=0.1573\tacc=0.9439\t8562.2 examples/second\n","[Step=3850]\tLoss=0.1564\tacc=0.9451\t9024.6 examples/second\n","[Step=3900]\tLoss=0.1545\tacc=0.9458\t8713.6 examples/second\n","Test Loss=0.3322, Test acc=0.8971\n","Accuracy AFTER finetuning:\n","Test Loss=0.3282, Test accuracy=0.8994\n","\n","=== Pruned + Quantized Model (Nbits = 3) ===\n","Accuracy BEFORE finetuning:\n","Test Loss=0.8701, Test accuracy=0.7186\n","==> Preparing data..\n","\n","Epoch: 0\n","[Step=50]\tLoss=2.0423\tacc=0.4129\t6084.3 examples/second\n","[Step=100]\tLoss=1.6499\tacc=0.4872\t9072.3 examples/second\n","[Step=150]\tLoss=1.4571\tacc=0.5330\t8999.2 examples/second\n","Test Loss=0.8651, Test acc=0.7018\n","Saving...\n","\n","Epoch: 1\n","[Step=200]\tLoss=0.8428\tacc=0.6963\t3759.0 examples/second\n","[Step=250]\tLoss=0.8070\tacc=0.7176\t8706.1 examples/second\n","[Step=300]\tLoss=0.7577\tacc=0.7372\t8652.8 examples/second\n","[Step=350]\tLoss=0.7200\tacc=0.7515\t8982.7 examples/second\n","Test Loss=0.6161, Test acc=0.7897\n","Saving...\n","\n","Epoch: 2\n","[Step=400]\tLoss=0.6007\tacc=0.7788\t3779.1 examples/second\n","[Step=450]\tLoss=0.5651\tacc=0.8033\t8570.7 examples/second\n","[Step=500]\tLoss=0.5483\tacc=0.8084\t8657.7 examples/second\n","[Step=550]\tLoss=0.5424\tacc=0.8107\t9139.6 examples/second\n","Test Loss=0.5599, Test acc=0.8056\n","Saving...\n","\n","Epoch: 3\n","[Step=600]\tLoss=0.5122\tacc=0.8193\t3771.1 examples/second\n","[Step=650]\tLoss=0.5047\tacc=0.8259\t9168.5 examples/second\n","[Step=700]\tLoss=0.4939\tacc=0.8299\t9263.7 examples/second\n","[Step=750]\tLoss=0.4842\tacc=0.8333\t8937.5 examples/second\n","Test Loss=0.5371, Test acc=0.8188\n","Saving...\n","\n","Epoch: 4\n","[Step=800]\tLoss=0.4674\tacc=0.8352\t3611.2 examples/second\n","[Step=850]\tLoss=0.4539\tacc=0.8435\t8725.5 examples/second\n","[Step=900]\tLoss=0.4495\tacc=0.8450\t8884.1 examples/second\n","[Step=950]\tLoss=0.4463\tacc=0.8451\t8552.9 examples/second\n","Test Loss=0.4966, Test acc=0.8308\n","Saving...\n","\n","Epoch: 5\n","[Step=1000]\tLoss=0.4263\tacc=0.8512\t3810.0 examples/second\n","[Step=1050]\tLoss=0.4084\tacc=0.8591\t8789.9 examples/second\n","[Step=1100]\tLoss=0.4075\tacc=0.8577\t8549.5 examples/second\n","[Step=1150]\tLoss=0.4069\tacc=0.8579\t8587.0 examples/second\n","Test Loss=0.4774, Test acc=0.8380\n","Saving...\n","\n","Epoch: 6\n","[Step=1200]\tLoss=0.3903\tacc=0.8654\t3810.9 examples/second\n","[Step=1250]\tLoss=0.3877\tacc=0.8645\t8458.5 examples/second\n","[Step=1300]\tLoss=0.3801\tacc=0.8672\t8647.8 examples/second\n","[Step=1350]\tLoss=0.3828\tacc=0.8666\t8711.9 examples/second\n","Test Loss=0.4993, Test acc=0.8352\n","\n","Epoch: 7\n","[Step=1400]\tLoss=0.3702\tacc=0.8725\t3673.7 examples/second\n","[Step=1450]\tLoss=0.3692\tacc=0.8718\t8394.9 examples/second\n","[Step=1500]\tLoss=0.3717\tacc=0.8718\t8812.3 examples/second\n","[Step=1550]\tLoss=0.3700\tacc=0.8728\t9236.8 examples/second\n","Test Loss=0.4606, Test acc=0.8459\n","Saving...\n","\n","Epoch: 8\n","[Step=1600]\tLoss=0.3571\tacc=0.8789\t3652.1 examples/second\n","[Step=1650]\tLoss=0.3573\tacc=0.8769\t8888.7 examples/second\n","[Step=1700]\tLoss=0.3511\tacc=0.8775\t8863.2 examples/second\n","[Step=1750]\tLoss=0.3500\tacc=0.8777\t9227.7 examples/second\n","Test Loss=0.4472, Test acc=0.8519\n","Saving...\n","\n","Epoch: 9\n","[Step=1800]\tLoss=0.3351\tacc=0.8810\t3657.8 examples/second\n","[Step=1850]\tLoss=0.3387\tacc=0.8804\t9064.6 examples/second\n","[Step=1900]\tLoss=0.3380\tacc=0.8819\t9192.2 examples/second\n","[Step=1950]\tLoss=0.3374\tacc=0.8823\t9485.3 examples/second\n","Test Loss=0.4433, Test acc=0.8525\n","Saving...\n","\n","Epoch: 10\n","[Step=2000]\tLoss=0.3353\tacc=0.8835\t3798.9 examples/second\n","[Step=2050]\tLoss=0.3356\tacc=0.8836\t8926.1 examples/second\n","[Step=2100]\tLoss=0.3326\tacc=0.8841\t8634.5 examples/second\n","[Step=2150]\tLoss=0.3279\tacc=0.8861\t9474.7 examples/second\n","Test Loss=0.4428, Test acc=0.8555\n","Saving...\n","\n","Epoch: 11\n","[Step=2200]\tLoss=0.3233\tacc=0.8878\t3735.6 examples/second\n","[Step=2250]\tLoss=0.3248\tacc=0.8880\t9163.2 examples/second\n","[Step=2300]\tLoss=0.3183\tacc=0.8895\t8590.0 examples/second\n","[Step=2350]\tLoss=0.3177\tacc=0.8905\t9324.9 examples/second\n","Test Loss=0.4330, Test acc=0.8595\n","Saving...\n","\n","Epoch: 12\n","[Step=2400]\tLoss=0.3175\tacc=0.8896\t3616.3 examples/second\n","[Step=2450]\tLoss=0.3126\tacc=0.8905\t8674.2 examples/second\n","[Step=2500]\tLoss=0.3113\tacc=0.8910\t9000.6 examples/second\n","Test Loss=0.4165, Test acc=0.8624\n","Saving...\n","\n","Epoch: 13\n","[Step=2550]\tLoss=0.3110\tacc=0.8887\t3839.9 examples/second\n","[Step=2600]\tLoss=0.3092\tacc=0.8891\t8683.6 examples/second\n","[Step=2650]\tLoss=0.3018\tacc=0.8925\t9245.5 examples/second\n","[Step=2700]\tLoss=0.3032\tacc=0.8927\t9052.6 examples/second\n","Test Loss=0.4194, Test acc=0.8635\n","Saving...\n","\n","Epoch: 14\n","[Step=2750]\tLoss=0.2895\tacc=0.8926\t3656.4 examples/second\n","[Step=2800]\tLoss=0.2936\tacc=0.8997\t9366.7 examples/second\n","[Step=2850]\tLoss=0.2930\tacc=0.8990\t9368.0 examples/second\n","[Step=2900]\tLoss=0.2918\tacc=0.8988\t9066.0 examples/second\n","Test Loss=0.4272, Test acc=0.8613\n","\n","Epoch: 15\n","[Step=2950]\tLoss=0.2844\tacc=0.9012\t3792.2 examples/second\n","[Step=3000]\tLoss=0.2920\tacc=0.8981\t8503.1 examples/second\n","[Step=3050]\tLoss=0.2917\tacc=0.8975\t8885.5 examples/second\n","[Step=3100]\tLoss=0.2901\tacc=0.8981\t9282.6 examples/second\n","Test Loss=0.4158, Test acc=0.8626\n","\n","Epoch: 16\n","[Step=3150]\tLoss=0.2792\tacc=0.9035\t3710.4 examples/second\n","[Step=3200]\tLoss=0.2800\tacc=0.9030\t8837.6 examples/second\n","[Step=3250]\tLoss=0.2796\tacc=0.9030\t8737.1 examples/second\n","[Step=3300]\tLoss=0.2826\tacc=0.9019\t8866.2 examples/second\n","Test Loss=0.3968, Test acc=0.8685\n","Saving...\n","\n","Epoch: 17\n","[Step=3350]\tLoss=0.2749\tacc=0.8991\t3595.7 examples/second\n","[Step=3400]\tLoss=0.2766\tacc=0.9008\t9142.1 examples/second\n","[Step=3450]\tLoss=0.2758\tacc=0.9007\t9354.4 examples/second\n","[Step=3500]\tLoss=0.2774\tacc=0.9001\t9273.3 examples/second\n","Test Loss=0.4113, Test acc=0.8661\n","\n","Epoch: 18\n","[Step=3550]\tLoss=0.2648\tacc=0.9066\t3726.7 examples/second\n","[Step=3600]\tLoss=0.2744\tacc=0.9036\t8805.4 examples/second\n","[Step=3650]\tLoss=0.2711\tacc=0.9057\t8738.8 examples/second\n","[Step=3700]\tLoss=0.2718\tacc=0.9055\t8760.1 examples/second\n","Test Loss=0.3981, Test acc=0.8685\n","\n","Epoch: 19\n","[Step=3750]\tLoss=0.2793\tacc=0.9019\t3889.4 examples/second\n","[Step=3800]\tLoss=0.2727\tacc=0.9048\t8666.4 examples/second\n","[Step=3850]\tLoss=0.2732\tacc=0.9045\t8835.1 examples/second\n","[Step=3900]\tLoss=0.2689\tacc=0.9060\t8853.6 examples/second\n","Test Loss=0.3986, Test acc=0.8715\n","Saving...\n","Accuracy AFTER finetuning:\n","Test Loss=0.3986, Test accuracy=0.8715\n","\n","=== Pruned + Quantized Model (Nbits = 2) ===\n","Accuracy BEFORE finetuning:\n","Test Loss=1705.2556, Test accuracy=0.1000\n","==> Preparing data..\n","\n","Epoch: 0\n","[Step=50]\tLoss=2.2960\tacc=0.1501\t6285.7 examples/second\n","[Step=100]\tLoss=2.2447\tacc=0.1586\t9102.8 examples/second\n","[Step=150]\tLoss=2.2215\tacc=0.1618\t9140.7 examples/second\n","Test Loss=2.1872, Test acc=0.1687\n","Saving...\n","\n","Epoch: 1\n","[Step=200]\tLoss=2.1560\tacc=0.1719\t3658.5 examples/second\n","[Step=250]\tLoss=2.1515\tacc=0.1853\t8417.0 examples/second\n","[Step=300]\tLoss=2.1453\tacc=0.1870\t8595.3 examples/second\n","[Step=350]\tLoss=2.1364\tacc=0.1923\t8645.1 examples/second\n","Test Loss=2.1085, Test acc=0.2041\n","Saving...\n","\n","Epoch: 2\n","[Step=400]\tLoss=2.1254\tacc=0.2100\t3749.6 examples/second\n","[Step=450]\tLoss=2.1090\tacc=0.2144\t9063.9 examples/second\n","[Step=500]\tLoss=2.1090\tacc=0.2163\t8963.4 examples/second\n","[Step=550]\tLoss=2.1018\tacc=0.2208\t8560.0 examples/second\n","Test Loss=2.0878, Test acc=0.2352\n","Saving...\n","\n","Epoch: 3\n","[Step=600]\tLoss=2.0841\tacc=0.2350\t3684.2 examples/second\n","[Step=650]\tLoss=2.0900\tacc=0.2235\t8713.4 examples/second\n","[Step=700]\tLoss=2.0818\tacc=0.2250\t9116.9 examples/second\n","[Step=750]\tLoss=2.0775\tacc=0.2312\t8957.4 examples/second\n","Test Loss=2.0859, Test acc=0.2261\n","\n","Epoch: 4\n","[Step=800]\tLoss=2.0737\tacc=0.2478\t3750.7 examples/second\n","[Step=850]\tLoss=2.0592\tacc=0.2507\t8585.7 examples/second\n","[Step=900]\tLoss=2.0539\tacc=0.2510\t8722.7 examples/second\n","[Step=950]\tLoss=2.0512\tacc=0.2505\t9181.3 examples/second\n","Test Loss=2.0856, Test acc=0.2380\n","Saving...\n","\n","Epoch: 5\n","[Step=1000]\tLoss=2.0286\tacc=0.2660\t3723.6 examples/second\n","[Step=1050]\tLoss=2.0366\tacc=0.2537\t9318.8 examples/second\n","[Step=1100]\tLoss=2.0325\tacc=0.2545\t9061.0 examples/second\n","[Step=1150]\tLoss=2.0302\tacc=0.2555\t9375.2 examples/second\n","Test Loss=2.0456, Test acc=0.2497\n","Saving...\n","\n","Epoch: 6\n","[Step=1200]\tLoss=2.0267\tacc=0.2565\t3693.1 examples/second\n","[Step=1250]\tLoss=2.0190\tacc=0.2613\t8945.6 examples/second\n","[Step=1300]\tLoss=2.0156\tacc=0.2621\t9095.2 examples/second\n","[Step=1350]\tLoss=2.0141\tacc=0.2614\t9292.5 examples/second\n","Test Loss=2.0143, Test acc=0.2659\n","Saving...\n","\n","Epoch: 7\n","[Step=1400]\tLoss=1.9978\tacc=0.2669\t3770.1 examples/second\n","[Step=1450]\tLoss=1.9939\tacc=0.2692\t8613.1 examples/second\n","[Step=1500]\tLoss=1.9918\tacc=0.2707\t8285.3 examples/second\n","[Step=1550]\tLoss=1.9941\tacc=0.2684\t8828.9 examples/second\n","Test Loss=1.9927, Test acc=0.2647\n","\n","Epoch: 8\n","[Step=1600]\tLoss=1.9830\tacc=0.2705\t3780.2 examples/second\n","[Step=1650]\tLoss=1.9788\tacc=0.2710\t9013.5 examples/second\n","[Step=1700]\tLoss=1.9784\tacc=0.2725\t8917.1 examples/second\n","[Step=1750]\tLoss=1.9765\tacc=0.2718\t9061.9 examples/second\n","Test Loss=2.0278, Test acc=0.2549\n","\n","Epoch: 9\n","[Step=1800]\tLoss=1.9583\tacc=0.2796\t3589.4 examples/second\n","[Step=1850]\tLoss=1.9623\tacc=0.2748\t8937.2 examples/second\n","[Step=1900]\tLoss=1.9573\tacc=0.2759\t8881.9 examples/second\n","[Step=1950]\tLoss=1.9582\tacc=0.2745\t9015.9 examples/second\n","Test Loss=1.9604, Test acc=0.2694\n","Saving...\n","\n","Epoch: 10\n","[Step=2000]\tLoss=1.9438\tacc=0.2749\t3730.3 examples/second\n","[Step=2050]\tLoss=1.9358\tacc=0.2814\t9011.4 examples/second\n","[Step=2100]\tLoss=1.9355\tacc=0.2834\t8693.3 examples/second\n","[Step=2150]\tLoss=1.9382\tacc=0.2817\t8922.6 examples/second\n","Test Loss=1.9743, Test acc=0.2563\n","\n","Epoch: 11\n","[Step=2200]\tLoss=1.9264\tacc=0.2888\t3731.9 examples/second\n","[Step=2250]\tLoss=1.9176\tacc=0.2921\t8953.8 examples/second\n","[Step=2300]\tLoss=1.9171\tacc=0.2903\t8988.0 examples/second\n","[Step=2350]\tLoss=1.9142\tacc=0.2914\t9480.9 examples/second\n","Test Loss=1.9755, Test acc=0.2659\n","\n","Epoch: 12\n","[Step=2400]\tLoss=1.8985\tacc=0.2969\t3677.2 examples/second\n","[Step=2450]\tLoss=1.8930\tacc=0.2962\t8426.6 examples/second\n","[Step=2500]\tLoss=1.8900\tacc=0.2985\t9066.3 examples/second\n","Test Loss=1.9661, Test acc=0.2753\n","Saving...\n","\n","Epoch: 13\n","[Step=2550]\tLoss=1.8962\tacc=0.2852\t3875.5 examples/second\n","[Step=2600]\tLoss=1.8785\tacc=0.2974\t8585.2 examples/second\n","[Step=2650]\tLoss=1.8693\tacc=0.3069\t9034.1 examples/second\n","[Step=2700]\tLoss=1.8682\tacc=0.3058\t9019.1 examples/second\n","Test Loss=1.9041, Test acc=0.2837\n","Saving...\n","\n","Epoch: 14\n","[Step=2750]\tLoss=1.8597\tacc=0.2839\t3710.9 examples/second\n","[Step=2800]\tLoss=1.8485\tacc=0.3071\t8834.1 examples/second\n","[Step=2850]\tLoss=1.8478\tacc=0.3112\t9079.9 examples/second\n","[Step=2900]\tLoss=1.8445\tacc=0.3109\t9293.0 examples/second\n","Test Loss=1.8673, Test acc=0.3041\n","Saving...\n","\n","Epoch: 15\n","[Step=2950]\tLoss=1.8374\tacc=0.3152\t3798.3 examples/second\n","[Step=3000]\tLoss=1.8186\tacc=0.3195\t8798.9 examples/second\n","[Step=3050]\tLoss=1.8194\tacc=0.3207\t8890.1 examples/second\n","[Step=3100]\tLoss=1.8149\tacc=0.3215\t8826.7 examples/second\n","Test Loss=1.8736, Test acc=0.2976\n","\n","Epoch: 16\n","[Step=3150]\tLoss=1.8080\tacc=0.3234\t3747.3 examples/second\n","[Step=3200]\tLoss=1.7969\tacc=0.3259\t9054.4 examples/second\n","[Step=3250]\tLoss=1.7924\tacc=0.3263\t9195.3 examples/second\n","[Step=3300]\tLoss=1.7897\tacc=0.3273\t9189.8 examples/second\n","Test Loss=1.8292, Test acc=0.3114\n","Saving...\n","\n","Epoch: 17\n","[Step=3350]\tLoss=1.7825\tacc=0.3333\t3648.6 examples/second\n","[Step=3400]\tLoss=1.7765\tacc=0.3339\t8525.2 examples/second\n","[Step=3450]\tLoss=1.7765\tacc=0.3354\t9032.2 examples/second\n","[Step=3500]\tLoss=1.7714\tacc=0.3357\t9307.0 examples/second\n","Test Loss=1.8018, Test acc=0.3131\n","Saving...\n","\n","Epoch: 18\n","[Step=3550]\tLoss=1.7658\tacc=0.3409\t3690.7 examples/second\n","[Step=3600]\tLoss=1.7622\tacc=0.3421\t8297.6 examples/second\n","[Step=3650]\tLoss=1.7569\tacc=0.3418\t9082.7 examples/second\n","[Step=3700]\tLoss=1.7537\tacc=0.3430\t8784.9 examples/second\n","Test Loss=1.9006, Test acc=0.3094\n","\n","Epoch: 19\n","[Step=3750]\tLoss=1.7542\tacc=0.3418\t3739.5 examples/second\n","[Step=3800]\tLoss=1.7421\tacc=0.3518\t9047.6 examples/second\n","[Step=3850]\tLoss=1.7393\tacc=0.3515\t8924.2 examples/second\n","[Step=3900]\tLoss=1.7315\tacc=0.3536\t9211.4 examples/second\n","Test Loss=1.7556, Test acc=0.3348\n","Saving...\n","Accuracy AFTER finetuning:\n","Test Loss=1.7556, Test accuracy=0.3348\n"]}]},{"cell_type":"markdown","source":["Finetuning improves the performance of quantized + pruned model, but lower bitwidths degrade more when combined with pruning."],"metadata":{"id":"phRuag5cGfOm"}},{"cell_type":"markdown","metadata":{"id":"5qXzauUHEuU1"},"source":["#### Symmetric Fixed-point quantization\n","Symmetric quantization is a commonly used and hardware-friendly quantization approach. In symmetric quantization, the quantization levels are symmetric to zero. Implement symmetric quantization in FP_layers.py"]},{"cell_type":"markdown","source":["##### Symmetric Quantization in FP_layers.py"],"metadata":{"id":"V1b_CzmqH5el"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","import numpy as np\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","class STE(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, w, bit, symmetric=False):\n","        '''\n","        symmetric: True for symmetric quantization, False for asymmetric quantization\n","        '''\n","        if bit is None:\n","            wq = w\n","        elif bit == 0:\n","            wq = w * 0\n","        else:\n","            # Build a mask to record position of zero weights\n","            weight_mask = (w != 0).float()\n","\n","            if symmetric == False:\n","                # Asymmetric quantization\n","                w_min = w.min()\n","                w_max = w.max()\n","                alpha = w_max - w_min\n","                beta = w_min\n","                ws = (w - beta) / (alpha + 1e-8)\n","                step = 2 ** bit - 1\n","                R = torch.round(ws * step) / step\n","                wq = R * alpha + beta\n","\n","            else:\n","                # Symmetric quantization\n","                w_absmax = torch.max(torch.abs(w))\n","                alpha = 2 * w_absmax\n","                ws = (w + w_absmax) / (alpha + 1e-8)  # Scale to [0, 1]\n","                step = 2 ** bit - 1\n","                R = torch.round(ws * step) / step\n","                wq = R * alpha - w_absmax\n","\n","            # Restore zero elements in wq\n","            wq = wq * weight_mask\n","\n","        return wq\n","\n","    @staticmethod\n","    def backward(ctx, g):\n","        return g, None, None\n","\n","class FP_Linear(nn.Module):\n","    def __init__(self, in_features, out_features, Nbits=None, symmetric=False):\n","        super(FP_Linear, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.linear = nn.Linear(in_features, out_features)\n","        self.Nbits = Nbits\n","        self.symmetric = symmetric\n","        m = self.in_features\n","        n = self.out_features\n","        self.linear.weight.data.normal_(0, math.sqrt(2. / (m + n)))\n","\n","    def forward(self, x):\n","        return F.linear(x, STE.apply(self.linear.weight, self.Nbits, self.symmetric), self.linear.bias)\n","\n","class FP_Conv(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=False, Nbits=None, symmetric=False):\n","        super(FP_Conv, self).__init__()\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.kernel_size = kernel_size\n","        self.stride = stride\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias)\n","        self.Nbits = Nbits\n","        self.symmetric = symmetric\n","        n = self.kernel_size * self.kernel_size * self.out_channels\n","        m = self.kernel_size * self.kernel_size * self.in_channels\n","        self.conv.weight.data.normal_(0, math.sqrt(2. / (n + m)))\n","        self.sparsity = 1.0\n","\n","    def forward(self, x):\n","        return F.conv2d(x, STE.apply(self.conv.weight, self.Nbits, self.symmetric), self.conv.bias, self.conv.stride, self.conv.padding, self.conv.dilation, self.conv.groups)"],"metadata":{"id":"HRZY2GvcG3wW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nJrzcB_tEuU1","executionInfo":{"status":"ok","timestamp":1743358902810,"user_tz":240,"elapsed":29474,"user":{"displayName":"Luopeiwen Yi","userId":"10397336716270013892"}},"outputId":"d1102d69-b91e-462f-84e6-0448add09be7"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Testing Symmetric Quantized ResNet-20 with Nbits = 6 (residual blocks only)\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170M/170M [00:13<00:00, 13.1MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss=0.3275, Test accuracy=0.9134\n","\n","Testing Symmetric Quantized ResNet-20 with Nbits = 5 (residual blocks only)\n","Test Loss=0.3419, Test accuracy=0.9071\n","\n","Testing Symmetric Quantized ResNet-20 with Nbits = 4 (residual blocks only)\n","Test Loss=0.5688, Test accuracy=0.8532\n","\n","Testing Symmetric Quantized ResNet-20 with Nbits = 3 (residual blocks only)\n","Test Loss=1.2517, Test accuracy=0.7151\n","\n","Testing Symmetric Quantized ResNet-20 with Nbits = 2 (residual blocks only)\n","Test Loss=96.5832, Test accuracy=0.1000\n"]}],"source":["# check the performance of symmetric quantization with 6, 5, 4, 3, 2 bits\n","bit_list = [6, 5, 4, 3, 2]\n","\n","for Nbits in bit_list:\n","    print(f\"\\nTesting Symmetric Quantized ResNet-20 with Nbits = {Nbits} (residual blocks only)\")\n","\n","    # Enable symmetric quantization in all residual blocks\n","    net = ResNetCIFAR(num_layers=20, Nbits=Nbits, symmetric=True)\n","    net.load_state_dict(torch.load(os.path.join(CODE_PATH, \"pretrained_model.pt\")))\n","    net = net.to(device)\n","\n","    test(net)"]},{"cell_type":"markdown","source":["Asymmetric quantization in general results in slightly lower accuracy than symmetric quantization (except the Nbits=2 where Asymmetric quantization results in slightly higher accuracy than symmetric quantization)."],"metadata":{"id":"4aJv7H7oV4RF"}}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"pycharm":{"stem_cell":{"cell_type":"raw","metadata":{"collapsed":false},"source":[]}},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}